{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a609cc5dd159025",
   "metadata": {},
   "source": [
    "## Use Custom Dataloading.py"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "352dfead4f947fa"
  },
  {
   "cell_type": "code",
   "id": "de2e39d37591a7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T22:06:44.502620Z",
     "start_time": "2025-10-09T22:06:44.450052Z"
    }
   },
   "source": [
    "from vae_scott import ResNetVAE\n",
    "from custom_dataloading import encode_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision\n",
    "import torch"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "c642d0ab71dbb7d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T05:04:49.160968Z",
     "start_time": "2025-10-09T05:04:48.990664Z"
    }
   },
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "vae = ResNetVAE(latent_dim=7, spatial=True).to(device)\n",
    "vae.load_state_dict(torch.load('models/spatial_ae_7.pth', map_location=device))\n",
    "vae.eval()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNetVAE(\n",
       "  (encoder): ResNetVAEEncoderSpatial(\n",
       "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (levels): ModuleList(\n",
       "      (0): ModuleList(\n",
       "        (0-3): 4 x ResidualBlock(\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): ModuleList(\n",
       "        (0-3): 4 x ResidualBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (2): ModuleList(\n",
       "        (0-3): 4 x ResidualBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (transitions): ModuleList(\n",
       "      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (channel_proj): Conv2d(128, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (decoder): ResNetVAEDecoderSpatial(\n",
       "    (channel_proj): Conv2d(1, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (levels): ModuleList(\n",
       "      (0): ModuleList(\n",
       "        (0-3): 4 x ResidualBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): ModuleList(\n",
       "        (0-3): 4 x ResidualBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (2): ModuleList(\n",
       "        (0-3): 4 x ResidualBlock(\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (transitions): ModuleList(\n",
       "      (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (final_conv): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "f0612f4e3adf0e2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T05:04:49.183964Z",
     "start_time": "2025-10-09T05:04:49.170190Z"
    }
   },
   "source": [
    "# load original dataset\n",
    "train_transforms = torchvision.transforms.Compose([ToTensor()])\n",
    "\n",
    "train_ds = MNIST(root='./data', train=True,  download=True, transform=train_transforms)\n",
    "test_ds  = MNIST(root='./data', train=False, download=True, transform=ToTensor())\n",
    "print(f\"Data set lengths: train: {len(train_ds)}, test: {len(test_ds)}\")\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 128   # could make this bigger; note for MNIST on Colab we're disk-speed limited, not GPU-limited\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, num_workers=2, shuffle=True, persistent_workers=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size, num_workers=2, shuffle=False, persistent_workers=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set lengths: train: 60000, test: 10000\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "f8c9b001c348d68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T05:04:57.630978Z",
     "start_time": "2025-10-09T05:04:51.684392Z"
    }
   },
   "source": "encode_dataset(vae, train_dl, test_dl, suffix='sp7')",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[encode] train -> data/ResNetVAEEncoderSpatial_sp7/MNIST/train | N=60000, latent=1x7x7, dtype=<class 'numpy.float32'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding train: 100%|██████████| 469/469 [00:03<00:00, 133.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[encode] test -> data/ResNetVAEEncoderSpatial_sp7/MNIST/test | N=10000, latent=1x7x7, dtype=<class 'numpy.float32'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding test: 100%|██████████| 79/79 [00:01<00:00, 55.75it/s]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "48a16337718356d4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Code for above func before it became .py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T20:01:01.112232Z",
     "start_time": "2025-10-06T20:01:01.109860Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from vae_scott import ResidualBlock, ResNetVAEEncoder, ResNetVAEDecoder, ResNetVAEEncoderSpatial, ResNetVAEDecoderSpatial, ResNetVAE\n",
    "from vae_scott import ResNetVAE, test_inference, test_inference_spatial, log_example_images\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim, utils\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor, RandomAffine, RandomErasing\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "from numpy.lib.format import open_memmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e11d268d3996a527",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T19:56:37.449983Z",
     "start_time": "2025-10-06T19:56:37.445841Z"
    }
   },
   "outputs": [],
   "source": [
    "class EncodedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_path, split=\"train\", add_channel=True, mmap=True):\n",
    "        \"\"\"\n",
    "        General dataset loader for encoded .npy latent datasets.\n",
    "\n",
    "        Args:\n",
    "            dataset_path (str): Path to dataset folder, e.g. '../data/ResNetVAEEncoderSpatial/MNIST'\n",
    "            split (str): 'train' or 'test'\n",
    "            add_channel (bool): Whether to add a channel dim (for (7,7) → (1,7,7))\n",
    "            mmap (bool): Whether to memory-map large files instead of loading into RAM\n",
    "\n",
    "        Path Structure\n",
    "        <encoder_name>/\n",
    "         └── <dataset_name>/\n",
    "              ├── train/\n",
    "              │    ├── data.npy\n",
    "              │    └── target.npy\n",
    "              └── test/\n",
    "                   ├── data.npy\n",
    "                   └── target.npy\n",
    "        \"\"\"\n",
    "        self.dataset_path = dataset_path\n",
    "        self.split = split\n",
    "        self.add_channel = add_channel\n",
    "\n",
    "        # Expected structure: <dataset_path>/<split>/data.npy + target.npy\n",
    "        base = os.path.join(dataset_path, split)\n",
    "        self.data_path = os.path.join(base, \"data.npy\")\n",
    "        self.target_path = os.path.join(base, \"target.npy\")\n",
    "\n",
    "        if not os.path.exists(self.data_path) or not os.path.exists(self.target_path):\n",
    "            raise FileNotFoundError(f\"Expected data.npy and target.npy in: {base}\")\n",
    "\n",
    "        mmap_mode = \"r\" if mmap else None\n",
    "        self.data = np.load(self.data_path, mmap_mode=mmap_mode)\n",
    "        self.targets = np.load(self.target_path, mmap_mode=mmap_mode)\n",
    "\n",
    "        assert len(self.data) == len(self.targets), \"Data and targets must be same length\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        if self.add_channel and x.ndim == 2:\n",
    "            x = np.expand_dims(x, 0)  # (1, H, W)\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7880517b8ac00be1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T19:45:17.393474Z",
     "start_time": "2025-10-06T19:45:17.235080Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNetVAE(\n",
       "  (encoder): ResNetVAEEncoderSpatial(\n",
       "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (levels): ModuleList(\n",
       "      (0): ModuleList(\n",
       "        (0-3): 4 x ResidualBlock(\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): ModuleList(\n",
       "        (0-3): 4 x ResidualBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (2): ModuleList(\n",
       "        (0-3): 4 x ResidualBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (transitions): ModuleList(\n",
       "      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (channel_proj): Conv2d(128, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (decoder): ResNetVAEDecoderSpatial(\n",
       "    (channel_proj): Conv2d(1, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (levels): ModuleList(\n",
       "      (0): ModuleList(\n",
       "        (0-3): 4 x ResidualBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): ModuleList(\n",
       "        (0-3): 4 x ResidualBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (2): ModuleList(\n",
       "        (0-3): 4 x ResidualBlock(\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (transitions): ModuleList(\n",
       "      (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (final_conv): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "ae_spatial_49 = ResNetVAE(latent_dim=49).to(device)\n",
    "ae_spatial_49.load_state_dict(torch.load('models/spatial_ae_49.pth', map_location=device))\n",
    "ae_spatial_49.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ea1b4256ef82d20a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T19:45:21.415275Z",
     "start_time": "2025-10-06T19:45:21.393701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set lengths: train: 60000, test: 10000\n"
     ]
    }
   ],
   "source": [
    "train_transforms = torchvision.transforms.Compose([\n",
    "    ToTensor(),\n",
    "    # uncomment next lines for extra augmentations\n",
    "    #RandomAffine(degrees=15, translate=(0.1, 0.1)),\n",
    "    #RandomErasing(p=0.2, scale=(0.02, 0.1))\n",
    "])\n",
    "\n",
    "train_ds = MNIST(root='./data', train=True,  download=True, transform=train_transforms)\n",
    "test_ds  = MNIST(root='./data', train=False, download=True, transform=ToTensor())\n",
    "print(f\"Data set lengths: train: {len(train_ds)}, test: {len(test_ds)}\")\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 128   # could make this bigger; note for MNIST on Colab we're disk-speed limited, not GPU-limited\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, num_workers=2, shuffle=True, persistent_workers=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size, num_workers=2, shuffle=False, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "4b41d5c116961894",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T20:01:09.853851Z",
     "start_time": "2025-10-06T20:01:09.848824Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_latent_dtype_dim(encoder, loader):\n",
    "    encoder.eval()  # Safety eval\n",
    "\n",
    "    x, _ = next(iter(loader))\n",
    "    x = x.to(device, dtype=torch.float32)\n",
    "    mu, _ = encoder(x)\n",
    "    return mu.dtype, tuple(mu.squeeze(1).shape[1:])\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_dataset(model, train_dataloader, test_dataloader, dir_='data/'):\n",
    "    # Some cheap checks\n",
    "    assert hasattr(model, \"encoder\"), \"Model must have an encoder attribute\"\n",
    "    assert model.encoder is not None, \"Model encoder cannot be None\"\n",
    "\n",
    "    encoder = model.encoder\n",
    "    encoder.eval()\n",
    "\n",
    "    model_dir = model.encoder.__class__.__name__\n",
    "\n",
    "    for loader, mode in [(train_dataloader, 'train'), (test_dataloader, 'test')]:\n",
    "        print(f\"Processing {mode} split...\\n\")\n",
    "        dataset_dir = loader.dataset.__class__.__name__\n",
    "        base_dir = os.path.join(dir_, model_dir, dataset_dir, mode)\n",
    "        os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "        batches = len(loader)\n",
    "        batch = i = 0\n",
    "        pbar = tqdm(loader, desc=f\"Loader: {dataset_dir} {mode}\", total=batches)\n",
    "\n",
    "        latent_torch_dtype, latent_img_dim = get_latent_dtype_dim(encoder, loader)\n",
    "        latent_torch_dtype = str(latent_torch_dtype).replace(\"torch.\", \"\")\n",
    "        dtype_target = str(loader.dataset.targets.dtype).replace(\"torch.\", \"\")\n",
    "        N = loader.dataset.data.shape[0]\n",
    "\n",
    "        data_mm   = open_memmap(os.path.join(base_dir, \"data.npy\"),   mode=\"w+\", dtype=latent_torch_dtype, shape=(N, *latent_img_dim))\n",
    "        target_mm = open_memmap(os.path.join(base_dir, \"target.npy\"), mode=\"w+\", dtype=dtype_target, shape=(N,))\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(pbar):\n",
    "            data = data.to(device)\n",
    "\n",
    "            mu, _ = encoder(data)\n",
    "            mu = mu.squeeze(1)\n",
    "\n",
    "            b = mu.size(0)  # latent_batch_size\n",
    "            data_mm[i:i+b] = mu.detach().cpu().numpy()\n",
    "            target_mm[i:i+b] = target.detach().cpu().numpy()\n",
    "            i += b\n",
    "\n",
    "            batch += 1\n",
    "            pbar.set_description(f'Loader: {dataset_dir} {mode} Batch {batch_idx+1}/{batches}')  # Update pbar\n",
    "\n",
    "        # Finish write\n",
    "        data_mm.flush()\n",
    "        target_mm.flush()\n",
    "        pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "16cc0deef3dd6c4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T20:01:18.798351Z",
     "start_time": "2025-10-06T20:01:14.386854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train split...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loader: MNIST train Batch 469/469: 100%|██████████| 469/469 [00:03<00:00, 124.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test split...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loader: MNIST test Batch 79/79: 100%|██████████| 79/79 [00:00<00:00, 121.91it/s]\n"
     ]
    }
   ],
   "source": [
    "encode_dataset(ae_spatial_49, train_dl, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "96a045c88c70d747",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T20:01:37.606939Z",
     "start_time": "2025-10-06T20:01:37.592415Z"
    }
   },
   "outputs": [],
   "source": [
    "root = \"data/ResNetVAEEncoderSpatial/MNIST\"\n",
    "\n",
    "train_ds = EncodedDataset(root, split=\"train\")\n",
    "test_ds  = EncodedDataset(root, split=\"test\")\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=128, shuffle=True, pin_memory=True)\n",
    "test_dl  = DataLoader(test_ds, batch_size=128, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c0a719b49c703ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T17:46:56.130211Z",
     "start_time": "2025-10-06T17:46:56.087681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x129373610>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "img = x_batch[0].squeeze()\n",
    "print(y_batch[0].item())\n",
    "plt.imshow(img, cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
