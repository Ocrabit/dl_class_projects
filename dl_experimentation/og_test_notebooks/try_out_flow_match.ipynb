{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "10a0c0d1bd7d35fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T17:17:37.364926Z",
     "start_time": "2025-10-08T17:17:37.361927Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import FlaubertModel\n",
    "\n",
    "from custom_dataloading import load_encoded_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import wandb\n",
    "import torch\n",
    "from torch import nn, optim, utils\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "36decbcafe244d4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T04:32:32.227198Z",
     "start_time": "2025-10-08T04:32:32.217926Z"
    }
   },
   "outputs": [],
   "source": [
    "from safetensors.torch import save_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "298cba1ed1b97749",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T04:35:37.417624Z",
     "start_time": "2025-10-08T04:35:37.220387Z"
    }
   },
   "outputs": [],
   "source": [
    "tensors = torch.load(\"models/vae_7.pth\")\n",
    "save_file(tensors, 'models/safetensors/vae_7.safetensors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b2b07730-6151-4f97-9060-e74d778b70f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T17:17:52.951061Z",
     "start_time": "2025-10-08T17:17:52.948504Z"
    }
   },
   "outputs": [],
   "source": [
    "latent_dim = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "59b59383b1deceba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T23:32:56.527331Z",
     "start_time": "2025-10-08T23:32:56.337751Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNetVAE(\n",
       "  (encoder): ResNetVAEEncoder(\n",
       "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (levels): ModuleList(\n",
       "      (0): ModuleList(\n",
       "        (0-3): 4 x ResidualBlock(\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): ModuleList(\n",
       "        (0-3): 4 x ResidualBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (2): ModuleList(\n",
       "        (0-3): 4 x ResidualBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (transitions): ModuleList(\n",
       "      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (global_avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (fc): Linear(in_features=128, out_features=14, bias=True)\n",
       "  )\n",
       "  (decoder): ResNetVAEDecoder(\n",
       "    (fc): Linear(in_features=7, out_features=6272, bias=True)\n",
       "    (levels): ModuleList(\n",
       "      (0): ModuleList(\n",
       "        (0-3): 4 x ResidualBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): ModuleList(\n",
       "        (0-3): 4 x ResidualBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (2): ModuleList(\n",
       "        (0-3): 4 x ResidualBlock(\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (transitions): ModuleList(\n",
       "      (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (final_conv): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vae_scott import ResNetVAE\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "vae = ResNetVAE(latent_dim=latent_dim, spatial=False).to(device)\n",
    "vae.load_state_dict(torch.load(f'models/vae_{latent_dim}.pth', map_location=device))\n",
    "vae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "16cb0f5e81623104",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T23:33:05.846504Z",
     "start_time": "2025-10-08T23:33:05.828301Z"
    }
   },
   "outputs": [],
   "source": [
    "from custom_dataloading import load_encoded_dataset\n",
    "\n",
    "train_ds, test_ds = load_encoded_dataset(f'data/ResNetVAEEncoder_ld{latent_dim}/MNIST/')\n",
    "batch_size = 128\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_dl  = DataLoader(test_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e2026de9-3e84-47a6-bf23-3fe779a30ecb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T23:33:07.272770Z",
     "start_time": "2025-10-08T23:33:07.269773Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 7)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dl.dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4aec6803c7b0fb7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T23:33:42.366980Z",
     "start_time": "2025-10-08T23:33:42.358672Z"
    }
   },
   "outputs": [],
   "source": [
    "for data, target in train_dl:\n",
    "    data.to(device)\n",
    "    sampled_x = torch.randn_like(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a1ac9815b3590fcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T23:35:40.083044Z",
     "start_time": "2025-10-08T23:35:40.080879Z"
    }
   },
   "outputs": [],
   "source": [
    "from flow_helpers import integrate_path, rk4_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "34d27f85dd54b6b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T23:34:52.526120Z",
     "start_time": "2025-10-08T23:34:52.523730Z"
    }
   },
   "outputs": [],
   "source": [
    "model = FlatVelocityNet(latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f310a7da01884a04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T23:35:24.033186Z",
     "start_time": "2025-10-08T23:35:24.020123Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f'models/flow_{latent_dim}.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7b452644579687e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T23:49:36.340495Z",
     "start_time": "2025-10-08T23:49:36.337113Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def spatial_integrate_path(model, initial_points, step_fn=rk4_step, n_steps=100,\n",
    "                   save_trajectories=False, warp_fn=None):\n",
    "    \"\"\"this 'sampling' routine is primarily used for visualization.\"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    model_dtype = next(model.parameters()).dtype\n",
    "\n",
    "    y = initial_points.to(device=device, dtype=model_dtype).clone()\n",
    "\n",
    "    ts = torch.linspace(0, 1, n_steps, device=device, dtype=model_dtype)\n",
    "    if warp_fn: ts = warp_fn(ts)\n",
    "\n",
    "    def f(y_, t_scalar):\n",
    "        B = y_.shape[0]\n",
    "        # Make a per-batch t vector; float dtype usually expected\n",
    "        t_vec = torch.full((B,), float(t_scalar), device=y_.device, dtype=y_.dtype)\n",
    "\n",
    "        if y_.dim() == 2:  # [B, D] non-spatial\n",
    "            return model(y_, t_vec)  # -> [B, D]\n",
    "        elif y_.dim() == 3:  # [B, H, W] -> assume C=1\n",
    "            out = model(y_.unsqueeze(1), t_vec)  # -> [B, 1, H, W]\n",
    "            return out.squeeze(1)  # -> [B, H, W]\n",
    "        elif y_.dim() == 4:  # [B, C, H, W]\n",
    "            return model(y_, t_vec)  # -> [B, C, H, W]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported state shape {y_.shape}\")\n",
    "\n",
    "    if save_trajectories: trajectories = [y.detach().clone()]\n",
    "\n",
    "    for i in range(len(ts) - 1):\n",
    "        t, dt = ts[i], ts[i + 1] - ts[i]\n",
    "        y = step_fn(f, y, t, dt)\n",
    "        if save_trajectories: trajectories.append(y.detach().clone())\n",
    "    if save_trajectories: return y, torch.stack(trajectories).cpu()\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5a89fce1af75fd6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T23:50:04.713543Z",
     "start_time": "2025-10-08T23:50:04.710475Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 7])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "85a3472e1a7812be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T23:57:42.175352Z",
     "start_time": "2025-10-08T23:57:42.170749Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def integrate_path(model, initial_points, step_fn=rk4_step, n_steps=100, save_trajectories=False, warp_fn=None):\n",
    "    \"\"\"\n",
    "    Unified integrator for both:\n",
    "      - non-spatial: [B, D]\n",
    "      - spatial: [B, C, H, W]  (or [B, H, W], treated as C=1)\n",
    "\n",
    "    Assumes model forward is: model(z, t_vec) -> same shape as z (or [B,1,H,W] for [B,H,W]).\n",
    "    \"\"\"\n",
    "    p = next(model.parameters())\n",
    "    device, model_dtype = p.device, p.dtype\n",
    "\n",
    "    y = initial_points.to(device=device, dtype=model_dtype).clone()\n",
    "    model.eval()\n",
    "\n",
    "    ts = torch.linspace(0, 1, n_steps, device=device, dtype=model_dtype)\n",
    "    if warp_fn:\n",
    "        ts = warp_fn(ts)\n",
    "\n",
    "\n",
    "    t_cache = {\"tensor\": None, \"B\": None}  # use to cache tensor shape\n",
    "\n",
    "    def adapted_model(y_, t_scalar):\n",
    "        B = y_.shape[0]\n",
    "\n",
    "        if t_cache[\"tensor\"] is None or t_cache[\"B\"] != B or t_cache[\"tensor\"].device != y_.device:\n",
    "            t_cache[\"tensor\"] = torch.empty((B, 1), device=y_.device, dtype=y_.dtype)\n",
    "            t_cache[\"B\"] = B\n",
    "        t_vec = t_cache[\"tensor\"]\n",
    "        t_vec.fill_(float(t_scalar))  # in-place fill\n",
    "        if y_.dim() == 2:                      # [B, D]\n",
    "            return model(y_, t_vec)            # -> [B, D]\n",
    "        elif y_.dim() == 3:                    # [B, H, W]  (assume C=1)\n",
    "            out = model(y_.unsqueeze(1), t_vec)  # -> [B, 1, H, W]\n",
    "            return out.squeeze(1)                 # -> [B, H, W]\n",
    "        elif y_.dim() == 4:                    # [B, C, H, W]\n",
    "            return model(y_, t_vec)            # -> [B, C, H, W]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported state shape {tuple(y_.shape)}\")\n",
    "\n",
    "    if save_trajectories: trajectories = [y.detach().clone()]\n",
    "\n",
    "    for i in range(len(ts) - 1):\n",
    "        t, dt = ts[i], ts[i + 1] - ts[i]\n",
    "        y = step_fn(adapted_model, y, t, dt)\n",
    "        if save_trajectories: trajectories.append(y.detach().clone())\n",
    "    if save_trajectories: return y, torch.stack(trajectories).cpu()\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "757207a17ab62ddb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T23:58:03.694583Z",
     "start_time": "2025-10-08T23:58:03.691573Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 7])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8edbac9b3ef9af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T23:57:52.916903Z",
     "start_time": "2025-10-08T23:57:52.904461Z"
    }
   },
   "outputs": [],
   "source": [
    "sampled_x = torch.randn_like(data)\n",
    "t = torch.rand(sampled_x.size(0), 1, device=device)\n",
    "integrate_path(model, sampled_x, n_steps=10, step_fn=rk4_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b2d10aa3-1a07-42b5-bc41-3ec273c2d348",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T23:34:35.477901Z",
     "start_time": "2025-10-08T23:34:35.473291Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FlatVelocityNet(nn.Module):\n",
    "    def __init__(self, input_dim, h_dim=64):\n",
    "        super().__init__()\n",
    "        self.fc_in  = nn.Linear(input_dim + 1, h_dim)\n",
    "        self.fc2    = nn.Linear(h_dim, h_dim)\n",
    "        self.fc3    = nn.Linear(h_dim, h_dim)\n",
    "        self.fc_out = nn.Linear(h_dim, input_dim)\n",
    "    \n",
    "    def forward(self, x, t, act=F.gelu):\n",
    "        t = t.expand(x.size(0), 1)  # Ensure t has the correct dimensions\n",
    "        x = torch.cat([x, t], dim=1)\n",
    "        x = act(self.fc_in(x))\n",
    "        x = act(self.fc2(x))\n",
    "        x = act(self.fc3(x))\n",
    "        return self.fc_out(x)\n",
    "\n",
    "class SimpleFlowModel(nn.Module):\n",
    "    def __init__(self, latent_dim=3, n_hidden=32, n_layers=3, act=nn.LeakyReLU):\n",
    "        super(SimpleFlowModel, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(latent_dim+1, n_hidden), act(),\n",
    "            *[nn.Sequential(nn.Linear(n_hidden, n_hidden), act()) for _ in range(n_layers-1)],\n",
    "            nn.Linear(n_hidden, latent_dim),)\n",
    "\n",
    "    def forward(self, x, t, act=F.gelu):\n",
    "        t = t.expand(x.size(0), 1)  # Ensure t has the correct dimensions\n",
    "        x = torch.cat([x, t], dim=1)\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "173feeb4-fe6a-4b14-b1d3-ec80c8dbdd39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T04:20:53.129145Z",
     "start_time": "2025-10-08T04:20:53.122128Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch, numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_latent_tsne3d(\n",
    "    model,\n",
    "    dataloader,                # e.g. test_dl\n",
    "    n_samples=2000,\n",
    "    pca_dim=50,                # set None to skip PCA\n",
    "    perplexity=30,\n",
    "    learning_rate=200,\n",
    "    random_state=42,           # reproducible t-SNE\n",
    "    device=None,\n",
    "    title=\"Latent Space (t-SNE 3D)\"\n",
    "):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "\n",
    "    Zs, Ys = [], []\n",
    "    total = 0\n",
    "    for xb, yb in dataloader:\n",
    "        if total >= n_samples: break\n",
    "        xb = xb.to(device)\n",
    "        enc_out = model.encoder(xb)\n",
    "        if isinstance(enc_out, (tuple, list)) and len(enc_out) >= 2:\n",
    "            mu, logvar = enc_out[:2]\n",
    "            z = mu\n",
    "        else:\n",
    "            z = enc_out  # AE case\n",
    "\n",
    "        if z.dim() > 2:                    # [B,C,H,W] -> [B, C*H*W]\n",
    "            z = z.view(z.size(0), -1)\n",
    "\n",
    "        Zs.append(z.detach().cpu())\n",
    "        Ys.append(yb.detach().cpu())\n",
    "        total += z.size(0)\n",
    "\n",
    "    if not Zs:\n",
    "        print(\"No data found—check dataloader / n_samples.\")\n",
    "        if was_training: model.train()\n",
    "        return\n",
    "\n",
    "    Z = torch.cat(Zs, 0)[:n_samples].numpy()\n",
    "    Y = torch.cat(Ys, 0)[:n_samples].numpy()\n",
    "\n",
    "    # optional PCA pre-step (commonly recommended before t-SNE)\n",
    "    if pca_dim is not None and Z.shape[1] > pca_dim:\n",
    "        Z = PCA(n_components=pca_dim, random_state=random_state).fit_transform(Z)\n",
    "\n",
    "    Z_tsne = TSNE(\n",
    "        n_components=3,\n",
    "        perplexity=perplexity,\n",
    "        learning_rate=learning_rate,\n",
    "        init=\"pca\",\n",
    "        random_state=random_state,\n",
    "        n_iter=1000,\n",
    "        verbose=0\n",
    "    ).fit_transform(Z)\n",
    "\n",
    "    fig = px.scatter_3d(\n",
    "        x=Z_tsne[:,0], y=Z_tsne[:,1], z=Z_tsne[:,2],\n",
    "        color=Y.astype(str),\n",
    "        opacity=0.75, template=\"plotly_dark\"\n",
    "    )\n",
    "    fig.update_traces(marker=dict(size=3, line=dict(width=0)))\n",
    "    fig.update_layout(\n",
    "        width=900, height=650,\n",
    "        scene=dict(xaxis_title=\"tSNE-1\", yaxis_title=\"tSNE-2\", zaxis_title=\"tSNE-3\",\n",
    "                   aspectmode=\"cube\"),\n",
    "        title=title\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "    if was_training: model.train()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3f59b1e3-4d76-4fda-b00f-e99b782b7387",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T04:20:54.504308Z",
     "start_time": "2025-10-08T04:20:54.492879Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FlatVelocityNet(\n",
       "  (fc_in): Linear(in_features=8, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (fc_out): Linear(in_features=64, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow_model = FlatVelocityNet(latent_dim)\n",
    "flow_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6fe47e46-2651-40a3-a0e9-efa90bdc2552",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T04:20:55.783728Z",
     "start_time": "2025-10-08T04:20:55.778301Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def fwd_euler_step(model, current_points, current_t, dt):\n",
    "    velocity = model(current_points, current_t)\n",
    "    return current_points + velocity * dt \n",
    "@torch.no_grad()\n",
    "def rk4_step(f, y, t, dt):\n",
    "    # f: callable (y, t) -> dy/dt  (aka model forward haha)\n",
    "    k1 = f(y, t)\n",
    "    k2 = f(y + 0.5*dt*k1, t + 0.5*dt)\n",
    "    k3 = f(y + 0.5*dt*k2, t + 0.5*dt)\n",
    "    k4 = f(y + dt*k3,     t + dt)\n",
    "    return y + (dt/6)*(k1 + 2*k2 + 2*k3 + k4)\n",
    "    \n",
    "@torch.no_grad()\n",
    "def integrate_path(model, initial_points, step_fn=rk4_step, n_steps=100,\n",
    "                   save_trajectories=False, warp_fn=None):\n",
    "    \"\"\"this 'sampling' routine is primarily used for visualization.\"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    current_points = initial_points.clone()\n",
    "    ts =  torch.linspace(0,1,n_steps).to(device)\n",
    "    if warp_fn: ts = warp_fn(ts)\n",
    "    if save_trajectories: trajectories = [current_points]    \n",
    "    for i in range(len(ts)-1):\n",
    "        current_points = step_fn(model, current_points, ts[i], ts[i+1]-ts[i])\n",
    "        if save_trajectories: trajectories.append(current_points)\n",
    "    if save_trajectories: return current_points, torch.stack(trajectories).cpu()\n",
    "    return current_points \n",
    "\n",
    "@torch.no_grad()\n",
    "def warp_time(t, dt=None, s=.5):\n",
    "    \"\"\"Parametric Time Warping: s = slope in the middle. \n",
    "        s=1 is linear time, s < 1 goes slower near the middle, s>1 goes slower near the ends\n",
    "        s = 1.5 gets very close to the \"cosine schedule\", i.e. (1-cos(pi*t))/2, i.e. sin^2(pi/2*x)\"\"\"\n",
    "    if s<0 or s>1.5: raise ValueError(f\"s={s} is out of bounds.\")\n",
    "    tw = 4*(1-s)*t**3 + 6*(s-1)*t**2 + (3-2*s)*t \n",
    "    if dt:                           # warped time-step requested; use derivative\n",
    "        return tw,  dt * 12*(1-s)*t**2 + 12*(s-1)*t + (3-2*s) \n",
    "    return tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a26fb8cf-8725-4bea-9740-ec59adf8810c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T04:20:56.324728Z",
     "start_time": "2025-10-08T04:20:56.319699Z"
    }
   },
   "outputs": [],
   "source": [
    "project = 'spatial_vae_conv_testing'\n",
    "def train_flow_model(model, train_loader, test_loader, name='flow_model', epochs=20, lr=0.001, warp_fn=None):\n",
    "    wandb.finish()\n",
    "    wandb.init(project=project, name=name, reinit='finish_previous')\n",
    "    wandb.config.update({\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": train_loader.batch_size,\n",
    "        \"learning_rate\": lr,\n",
    "        \"model\": model.__class__.__name__,\n",
    "        \"optimizer\": \"Adam\"\n",
    "    })\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    global_step = 1\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "\n",
    "        for batch_idx, (data, _) in enumerate(pbar):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Get data and create a sample from it to calc movement\n",
    "            target_x = data.to(device)\n",
    "            sampled_x = torch.randn_like(target_x)\n",
    "            \n",
    "            t  = torch.rand(sampled_x.size(0), 1, device=device)\n",
    "            if warp_fn: t = warp_fn(t)\n",
    "\n",
    "            interpolated_x = sampled_x * (1 - t) + target_x * t\n",
    "            line_directions = target_x - sampled_x\n",
    "            \n",
    "            drift = model(interpolated_x, t)\n",
    "            loss = criterion(drift, line_directions)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                cos_sim = F.cosine_similarity(drift, line_directions, dim=1).mean()\n",
    "                \n",
    "            wandb.log({\n",
    "                \"step\": global_step,\n",
    "                \"train_loss\": loss.item(),\n",
    "                \"cos_sim\": cos_sim.item(),\n",
    "                \"drift\": drift.norm(dim=1).mean().item(),\n",
    "            })\n",
    "            global_step += 1\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7212f957-cc57-450a-90ab-d6e323c3b4c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T04:20:57.330176Z",
     "start_time": "2025-10-08T04:20:57.324323Z"
    }
   },
   "outputs": [],
   "source": [
    "project = 'spatial_vae_conv_testing'\n",
    "def train_reflow_model(model, train_loader, test_loader, pretrained_model=None, name='flow_model', epochs=20, lr=0.001, new_points_every=1, warp_fn=None):\n",
    "    # Move to device\n",
    "    device = next(model.parameters()).device\n",
    "    model.to(device)\n",
    "    if pretrained_model is not None:\n",
    "        pretrained_model.to(device)\n",
    "        pretrained_model.eval()\n",
    "    \n",
    "    wandb.finish()\n",
    "    wandb.init(project=project, name=name, reinit='finish_previous')\n",
    "    wandb.config.update({\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": train_loader.batch_size,\n",
    "        \"learning_rate\": lr,\n",
    "        \"model\": model.__class__.__name__,\n",
    "        \"optimizer\": \"Adam\"\n",
    "    })\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    global_step = 1\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "\n",
    "        for batch_idx, (data, _) in enumerate(pbar):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            target_x = data.to(device)\n",
    "\n",
    "            if global_step % new_points_every == 0:\n",
    "                sampled_x = torch.randn_like(target_x)\n",
    "                if pretrained_model:   # ReFlow\n",
    "                    target_x = integrate_path(pretrained_model, sampled_x, step_fn=rk4_step, warp_fn=warp_time, n_steps=20)\n",
    "            \n",
    "            t  = torch.rand(sampled_x.size(0), 1, device=device)\n",
    "            if warp_fn: t = warp_fn(t)\n",
    "\n",
    "            interpolated_x = sampled_x * (1 - t) + target_x * t\n",
    "            line_directions = target_x - sampled_x\n",
    "            \n",
    "            drift = model(interpolated_x, t)\n",
    "            loss = criterion(drift, line_directions)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                cos_sim = F.cosine_similarity(drift, line_directions, dim=1).mean()\n",
    "                \n",
    "            wandb.log({\n",
    "                \"step\": global_step,\n",
    "                \"train_loss\": loss.item(),\n",
    "                \"cos_sim\": cos_sim.item(),\n",
    "                \"drift\": drift.norm(dim=1).mean().item(),\n",
    "            })\n",
    "            global_step += 1\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fde90f461f533052",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T04:21:41.576315Z",
     "start_time": "2025-10-08T04:21:09.030016Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>cos_sim</td><td>▁███████████████████████████████████████</td></tr><tr><td>drift</td><td>▁▂▅▆▆▇▇█▇█▇█▇██▇███▇▇▇▇▇▇▇▇█▇▇█▇▇█▇▇▇██▇</td></tr><tr><td>step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>cos_sim</td><td>0.99811</td></tr><tr><td>drift</td><td>5.77149</td></tr><tr><td>step</td><td>9380</td></tr><tr><td>train_loss</td><td>0.02371</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">simple_reflow_7</strong> at: <a href='https://wandb.ai/marcocassar-belmont-university/spatial_vae_conv_testing/runs/8vodxp40' target=\"_blank\">https://wandb.ai/marcocassar-belmont-university/spatial_vae_conv_testing/runs/8vodxp40</a><br> View project at: <a href='https://wandb.ai/marcocassar-belmont-university/spatial_vae_conv_testing' target=\"_blank\">https://wandb.ai/marcocassar-belmont-university/spatial_vae_conv_testing</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251007_232158-8vodxp40/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/marcocassar/Projects/DLAIE/self/flow_experimentation/wandb/run-20251009_142635-jyveby3y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcocassar-belmont-university/spatial_vae_conv_testing/runs/jyveby3y' target=\"_blank\">flow:)_7</a></strong> to <a href='https://wandb.ai/marcocassar-belmont-university/spatial_vae_conv_testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcocassar-belmont-university/spatial_vae_conv_testing' target=\"_blank\">https://wandb.ai/marcocassar-belmont-university/spatial_vae_conv_testing</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcocassar-belmont-university/spatial_vae_conv_testing/runs/jyveby3y' target=\"_blank\">https://wandb.ai/marcocassar-belmont-university/spatial_vae_conv_testing/runs/jyveby3y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 469/469 [00:01<00:00, 329.94it/s]\n",
      "Epoch 2/20: 100%|██████████| 469/469 [00:01<00:00, 339.70it/s]\n",
      "Epoch 3/20: 100%|██████████| 469/469 [00:01<00:00, 367.18it/s]\n",
      "Epoch 4/20: 100%|██████████| 469/469 [00:01<00:00, 357.45it/s]\n",
      "Epoch 5/20: 100%|██████████| 469/469 [00:01<00:00, 359.49it/s]\n",
      "Epoch 6/20: 100%|██████████| 469/469 [00:01<00:00, 356.26it/s]\n",
      "Epoch 7/20: 100%|██████████| 469/469 [00:01<00:00, 361.35it/s]\n",
      "Epoch 8/20: 100%|██████████| 469/469 [00:01<00:00, 356.90it/s]\n",
      "Epoch 9/20: 100%|██████████| 469/469 [00:01<00:00, 321.34it/s]\n",
      "Epoch 10/20: 100%|██████████| 469/469 [00:01<00:00, 281.80it/s]\n",
      "Epoch 11/20: 100%|██████████| 469/469 [00:01<00:00, 354.09it/s]\n",
      "Epoch 12/20: 100%|██████████| 469/469 [00:01<00:00, 347.39it/s]\n",
      "Epoch 13/20: 100%|██████████| 469/469 [00:01<00:00, 357.64it/s]\n",
      "Epoch 14/20: 100%|██████████| 469/469 [00:01<00:00, 368.62it/s]\n",
      "Epoch 15/20: 100%|██████████| 469/469 [00:01<00:00, 351.54it/s]\n",
      "Epoch 16/20: 100%|██████████| 469/469 [00:01<00:00, 350.82it/s]\n",
      "Epoch 17/20: 100%|██████████| 469/469 [00:01<00:00, 355.20it/s]\n",
      "Epoch 18/20: 100%|██████████| 469/469 [00:01<00:00, 357.05it/s]\n",
      "Epoch 19/20: 100%|██████████| 469/469 [00:01<00:00, 350.91it/s]\n",
      "Epoch 20/20: 100%|██████████| 469/469 [00:01<00:00, 331.48it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FlatVelocityNet(\n",
       "  (fc_in): Linear(in_features=8, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (fc_out): Linear(in_features=64, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_flow_model(flow_model, train_dl, test_dl, name=f'flow:)_{latent_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c87d6992-9f21-464e-a2a6-ba48bc744d3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T03:54:30.501331Z",
     "start_time": "2025-10-08T03:53:55.301795Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarcocassar\u001b[0m (\u001b[33mmarcocassar-belmont-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "jetTransient": {
      "display_id": "9a57fae12467beb031ada837707d6159"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/marcocassar/Projects/DLAIE/self/flow_experimentation/wandb/run-20251007_225356-4jp7v86o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcocassar-belmont-university/spatial_vae_conv_testing/runs/4jp7v86o' target=\"_blank\">flow_vae_7</a></strong> to <a href='https://wandb.ai/marcocassar-belmont-university/spatial_vae_conv_testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcocassar-belmont-university/spatial_vae_conv_testing' target=\"_blank\">https://wandb.ai/marcocassar-belmont-university/spatial_vae_conv_testing</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcocassar-belmont-university/spatial_vae_conv_testing/runs/4jp7v86o' target=\"_blank\">https://wandb.ai/marcocassar-belmont-university/spatial_vae_conv_testing/runs/4jp7v86o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 469/469 [00:02<00:00, 179.26it/s]\n",
      "Epoch 2/20: 100%|██████████| 469/469 [00:01<00:00, 274.16it/s]\n",
      "Epoch 3/20: 100%|██████████| 469/469 [00:01<00:00, 283.09it/s]\n",
      "Epoch 4/20: 100%|██████████| 469/469 [00:01<00:00, 303.51it/s]\n",
      "Epoch 5/20: 100%|██████████| 469/469 [00:01<00:00, 284.50it/s]\n",
      "Epoch 6/20: 100%|██████████| 469/469 [00:01<00:00, 280.69it/s]\n",
      "Epoch 7/20: 100%|██████████| 469/469 [00:01<00:00, 286.78it/s]\n",
      "Epoch 8/20: 100%|██████████| 469/469 [00:01<00:00, 318.37it/s]\n",
      "Epoch 9/20: 100%|██████████| 469/469 [00:01<00:00, 317.10it/s]\n",
      "Epoch 10/20: 100%|██████████| 469/469 [00:01<00:00, 319.87it/s]\n",
      "Epoch 11/20: 100%|██████████| 469/469 [00:01<00:00, 316.97it/s]\n",
      "Epoch 12/20: 100%|██████████| 469/469 [00:01<00:00, 318.83it/s]\n",
      "Epoch 13/20: 100%|██████████| 469/469 [00:01<00:00, 318.03it/s]\n",
      "Epoch 14/20: 100%|██████████| 469/469 [00:02<00:00, 209.23it/s]\n",
      "Epoch 15/20: 100%|██████████| 469/469 [00:01<00:00, 253.76it/s]\n",
      "Epoch 16/20: 100%|██████████| 469/469 [00:01<00:00, 286.42it/s]\n",
      "Epoch 17/20: 100%|██████████| 469/469 [00:01<00:00, 281.50it/s]\n",
      "Epoch 18/20: 100%|██████████| 469/469 [00:01<00:00, 298.28it/s]\n",
      "Epoch 19/20: 100%|██████████| 469/469 [00:01<00:00, 278.17it/s]\n",
      "Epoch 20/20: 100%|██████████| 469/469 [00:01<00:00, 277.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FlatVelocityNet(\n",
       "  (fc_in): Linear(in_features=8, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (fc_out): Linear(in_features=64, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_flow_model(flow_model, train_dl, test_dl, name='flow_vae_7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "376cc59b867d7ac0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T04:24:43.966647Z",
     "start_time": "2025-10-08T04:24:43.961270Z"
    }
   },
   "outputs": [],
   "source": [
    "# torch.save(simple_flow.state_dict(), f\"models/simple_flow_{latent_dim}.pth\")\n",
    "torch.save(new_model.state_dict(), f\"models/simple_reflow_{latent_dim}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f2c5998c-948c-4ec1-a0bb-ce9c6e597141",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T04:24:43.949069Z",
     "start_time": "2025-10-08T04:21:57.627674Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "jetTransient": {
      "display_id": "37a5c0a328de31e4504352040a1ad937"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>cos_sim</td><td>▁▇█▆▇▇▇▇▇▇▇▇▇▇██▇▇█▇▇▆▇▇▇██▇▇▇▇▇▇▆▇▇▇▇▇▇</td></tr><tr><td>drift</td><td>▁▃▄▆▄▅▆▅▆▄▅▅▄▆▅▆▅▆▅█▆▅▇▆▆▆▆▇▇▇▅▅▅▅▆▆▆▆▆▆</td></tr><tr><td>step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train_loss</td><td>█▃▃▃▃▃▃▃▂▃▂▂▂▂▂▃▃▂▃▁▂▃▃▂▂▁▂▂▂▂▃▂▁▂▂▂▁▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>cos_sim</td><td>0.79415</td></tr><tr><td>drift</td><td>6.60859</td></tr><tr><td>step</td><td>9380</td></tr><tr><td>train_loss</td><td>3.08127</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">simple_flow_7</strong> at: <a href='https://wandb.ai/marcocassar-belmont-university/spatial_vae_conv_testing/runs/up2p9lza' target=\"_blank\">https://wandb.ai/marcocassar-belmont-university/spatial_vae_conv_testing/runs/up2p9lza</a><br> View project at: <a href='https://wandb.ai/marcocassar-belmont-university/spatial_vae_conv_testing' target=\"_blank\">https://wandb.ai/marcocassar-belmont-university/spatial_vae_conv_testing</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251007_232109-up2p9lza/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "jetTransient": {
      "display_id": "8f0c87e5233c7445a57c2b2840d88352"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/marcocassar/Projects/DLAIE/self/flow_experimentation/wandb/run-20251007_232158-8vodxp40</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcocassar-belmont-university/spatial_vae_conv_testing/runs/8vodxp40' target=\"_blank\">simple_reflow_7</a></strong> to <a href='https://wandb.ai/marcocassar-belmont-university/spatial_vae_conv_testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcocassar-belmont-university/spatial_vae_conv_testing' target=\"_blank\">https://wandb.ai/marcocassar-belmont-university/spatial_vae_conv_testing</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcocassar-belmont-university/spatial_vae_conv_testing/runs/8vodxp40' target=\"_blank\">https://wandb.ai/marcocassar-belmont-university/spatial_vae_conv_testing/runs/8vodxp40</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 469/469 [00:08<00:00, 57.36it/s]\n",
      "Epoch 2/20: 100%|██████████| 469/469 [00:08<00:00, 56.94it/s]\n",
      "Epoch 3/20: 100%|██████████| 469/469 [00:08<00:00, 57.16it/s]\n",
      "Epoch 4/20: 100%|██████████| 469/469 [00:08<00:00, 56.78it/s]\n",
      "Epoch 5/20: 100%|██████████| 469/469 [00:08<00:00, 56.97it/s]\n",
      "Epoch 6/20: 100%|██████████| 469/469 [00:08<00:00, 56.96it/s]\n",
      "Epoch 7/20: 100%|██████████| 469/469 [00:08<00:00, 57.06it/s]\n",
      "Epoch 8/20: 100%|██████████| 469/469 [00:08<00:00, 56.84it/s]\n",
      "Epoch 9/20: 100%|██████████| 469/469 [00:08<00:00, 56.68it/s]\n",
      "Epoch 10/20: 100%|██████████| 469/469 [00:08<00:00, 56.78it/s]\n",
      "Epoch 11/20: 100%|██████████| 469/469 [00:08<00:00, 56.77it/s]\n",
      "Epoch 12/20: 100%|██████████| 469/469 [00:08<00:00, 56.41it/s]\n",
      "Epoch 13/20: 100%|██████████| 469/469 [00:08<00:00, 56.08it/s]\n",
      "Epoch 14/20: 100%|██████████| 469/469 [00:08<00:00, 56.49it/s]\n",
      "Epoch 15/20: 100%|██████████| 469/469 [00:08<00:00, 56.86it/s]\n",
      "Epoch 16/20: 100%|██████████| 469/469 [00:08<00:00, 56.66it/s]\n",
      "Epoch 18/20: 100%|██████████| 469/469 [00:08<00:00, 56.58it/s]\n",
      "Epoch 19/20: 100%|██████████| 469/469 [00:08<00:00, 55.86it/s]\n",
      "Epoch 20/20: 100%|██████████| 469/469 [00:08<00:00, 56.61it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleFlowModel(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=32, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (4): Linear(in_features=32, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_flow_model = simple_flow\n",
    "new_model = SimpleFlowModel(latent_dim)\n",
    "new_model.to(device)\n",
    "train_reflow_model(new_model, train_dl, test_dl,pretrained_flow_model, name=f'simple_reflow_{latent_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "adb67a87-de35-4b18-af66-3fffd43094b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T02:36:51.820909Z",
     "start_time": "2025-10-08T02:36:51.815712Z"
    }
   },
   "outputs": [],
   "source": [
    "class MarcosModel(nn.Module):\n",
    "    def __init__(self, vae, flow_model, latent_dim=3):\n",
    "        super().__init__()\n",
    "\n",
    "        #--- REQUIRED INFO:\n",
    "        self.info = { \n",
    "            'team': 'Marco',  # REPLACE with your team name. This will be public\n",
    "            'names': 'Marco', # or single name. This will be kept private\n",
    "        }\n",
    "        self.latent_dim = latent_dim\n",
    "        self.device = 'mps'\n",
    "\n",
    "        #--- TEMP\n",
    "        self.vae = vae\n",
    "        self.flow_model = flow_model\n",
    "\n",
    "        # keep support for full auto-initialization:\n",
    "        # self.load_vae()\n",
    "        # self.load_flow_model()\n",
    "\n",
    "        # Integrate later\n",
    "    \n",
    "    def generate_samples(self, n_samples:int, n_steps=100) -> torch.Tensor:\n",
    "            z0 = torch.randn(n_samples, self.latent_dim, device=self.device)\n",
    "            z1 = integrate_path(self.flow_model, z0, n_steps=n_steps, step_fn=rk4_step)\n",
    "            gen_xhat = F.sigmoid(self.decode(z1).view(-1, 28, 28))\n",
    "            return gen_xhat\n",
    "\n",
    "    def encode(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        # if your vae has linear layers, flatten first\n",
    "        # if your vae has conv layers, comment out next line\n",
    "        # images = images.view(images.size(0), -1)  \n",
    "        with torch.no_grad():\n",
    "            z = self.vae.encoder(images.to(self.device))\n",
    "            mu = z[:, :self.latent_dim]  # return only first half (mu)\n",
    "            return mu\n",
    "    \n",
    "    def decode(self, latents: torch.Tensor) -> torch.Tensor:\n",
    "        return self.vae.decoder(latents)\n",
    "\n",
    "    def to(self, device):\n",
    "        self.device = device \n",
    "        self.vae.to(self.device)\n",
    "        self.flow_model.to(self.device)\n",
    "        return self "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3bc48a30-476a-41ef-9b30-8dcba4804f4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T02:36:55.050977Z",
     "start_time": "2025-10-08T02:36:55.046704Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_generated(gen_xhat, nrow=5):\n",
    "    gen_xhat = gen_xhat.detach().cpu()\n",
    "\n",
    "    # assume shape (N, 28, 28)\n",
    "    n_samples = gen_xhat.size(0)\n",
    "    ncol = nrow\n",
    "    nrow = (n_samples + ncol - 1) // ncol\n",
    "\n",
    "    fig, axes = plt.subplots(nrow, ncol, figsize=(ncol*2, nrow*2))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i < n_samples:\n",
    "            ax.imshow(gen_xhat[i], cmap=\"gray\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "376e990701f2300",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T02:37:09.778310Z",
     "start_time": "2025-10-08T02:37:09.775190Z"
    }
   },
   "outputs": [],
   "source": [
    "test_model = MarcosModel(vae, flow_model, latent_dim=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3052585c-656e-4b75-a90c-9e75a2680598",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 7]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[120]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     55\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m was_training:\n\u001b[32m     56\u001b[39m             model.train()\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[43mtest_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/DLAIE/self/flow_experimentation/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[120]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mtest_inference\u001b[39m\u001b[34m(model, test_ds, idx, return_fig)\u001b[39m\n\u001b[32m     18\u001b[39m     x_batch = x_batch.unsqueeze(\u001b[32m1\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# forward\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m result = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# unpack (assumes (z, recon_logits, mu, log_var, z_hat))\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/DLAIE/self/flow_experimentation/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/DLAIE/self/flow_experimentation/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/DLAIE/self/flow_experimentation/vae_scott.py:165\u001b[39m, in \u001b[36mResNetVAE.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     mu, log_var = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     z = torch.cat([mu, log_var], dim=\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# this is unnecessary/redundant but our other Lesson code expects z\u001b[39;00m\n\u001b[32m    167\u001b[39m     z_hat = mu + torch.exp(\u001b[32m0.5\u001b[39m * log_var) * torch.randn_like(mu)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/DLAIE/self/flow_experimentation/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/DLAIE/self/flow_experimentation/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/DLAIE/self/flow_experimentation/vae_scott.py:45\u001b[39m, in \u001b[36mResNetVAEEncoder.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.act()(\u001b[38;5;28mself\u001b[39m.bn1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.levels)):\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m i > \u001b[32m0\u001b[39m:  \u001b[38;5;66;03m# shrink down\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/DLAIE/self/flow_experimentation/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/DLAIE/self/flow_experimentation/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/DLAIE/self/flow_experimentation/.venv/lib/python3.13/site-packages/torch/nn/modules/conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/DLAIE/self/flow_experimentation/.venv/lib/python3.13/site-packages/torch/nn/modules/conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 7]"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d01f799a78c51e7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T02:37:54.244082Z",
     "start_time": "2025-10-08T02:37:54.137498Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_generated(test_model.generate_samples(10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "073b09d3-6705-488c-9112-c7134b31269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_worker = MarcosModel(vae, new_model, latent_dim=latent_dim)\n",
    "model_worker = MarcosModel(vae, pretrained_flow_model, latent_dim=latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "a9904c27-6b36-4ac2-891e-9acd5d75b519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Samples\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA9wAAAGGCAYAAACJ2omlAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAL09JREFUeJzt3Xm0VvV1P36Z5xlBZAZFQBQ0zlNQkBINNVajcVym1iTG2C6zamNWTdK1agczNE2jsWlTjTTJMqlWTVRUjFRrnE1BlFFQBJnnGe4Ffuv561f7/ewjnwsf4XJfrz/34z73uZdzznO2z1r73WzPnj17DgMAAAD2q+b793AAAABAjYEbAAAACjBwAwAAQAEGbgAAACjAwA0AAAAFGLgBAACgAAM3AAAAFGDgBgAAgAIM3AAAAFBAy739D5s1a1bi58MBs2fPnn3qd01wqHFNwIe5JuDDXBOQf034hhsAAAAKMHADAABAAQZuAAAAKMDADQAAAAUYuAEAAKAAAzcAAAAUYOAGAACAAgzcAAAAUICBGwAAAAowcAMAAEABBm4AAAAowMANAAAABRi4AQAAoAADNwAAABRg4AYAAIACDNwAAABQgIEbAAAACjBwAwAAQAEtSxyUxq1Zs2b77Vh79uzZb8eCxnStNG/ePLtn9+7dWXUAAA5uvuEGAACAAgzcAAAAUICBGwAAAAowcAMAAEABBm4AAAAowMANAAAABYgFO8R17tw5Wb/qqqvCni996UvJetu2bcOep59+Oln/wQ9+EPa8++67yfquXbvCHtibiK0okqumTZs2yXq3bt3CnkGDBiXr/fv3z772tm3bFvYsXLgwWZ89e3bYs27dumRdJB9A/DnRsmX8CNyhQ4dkvV27dmFPdG/fvHlz2FNfXx++Bhw6fMMNAAAABRi4AQAAoAADNwAAABRg4AYAAIACDNwAAABQgC3lh4BWrVqFrx1//PHJ+jXXXBP2DBkyJPs9RFucO3bsGPZEm6RtKW+aoo2x0bbYmn79+iXrI0eODHui14YOHRr29OnTJ1lv37592LN79+5kff369WHPnDlzshMCfve73yXr27dvP6ypadGiRfhaz549k/URI0aEPcOGDUvW+/btG/Z07do1WW/dunXYU1dXl6yvXbs2WV+wYEF4rFmzZiXrixYtCns2bNiQvUHZFnwOpuedY445Juy5/PLLk/WJEyeGPb169cpOwFizZk2yPnXq1LDnn//5n7NSXKo+W4CDl2+4AQAAoAADNwAAABRg4AYAAIACDNwAAABQgIEbAAAACjBwAwAAQAFiwRqRZs2aJevdunULey666KJk/fDDD8/++VVRFOvWrUvWN23a1KDjcWieqzVt2rTJiqObMGFCeKxzzz03WR8+fHjY06NHj2S9Xbt22XFTVdFIUaTS5s2bw57u3bsn66tWrQp7pk+f3uRiwaJooPHjx4c9X/va17JjwaLYt6ooxig2qOqaiM6j6B65c+fO7IivKHKu5j//8z+T9d/85jdhz7Jly5J1sY7sD1GM3plnnpms33rrreGxzjjjjOxYx4Y8t0SRgFWfLUuXLk3W77333uxrnENb9NkSxZZ++tOfDo911llnZf2MmnfeeScrmrTmjTfeyJoZDuXZwDfcAAAAUICBGwAAAAowcAMAAEABBm4AAAAowMANAAAABdhS3ohEm3GPPvrosKdz587ZW4+jTZtVm3F///vfJ+tr1qxpcpsIm5Jo83LVVtYTTzwxWb/uuuuS9XHjxoXH6t27d9aG2/29RbpqS3n0c9q2bRv2dOjQIbsn2qB+KIs2zX/2s58Ne0aNGpWsd+nSJftvW3WufByiTf81HTt2zLpWaoYNG5asH3nkkWHPj370o2R9xYoVYY97Pnu77X/s2LHJ+re+9a1k/aSTTgqP1bJl+lF3x44dYc/GjRuz0x+i6zJKn6j51Kc+laxPnTo17InSX1xfjV/VlvDoM+y2225L1v/gD/4gPFa0ob+uri7siRJWzj///LDn4YcfTtYnT54c9qxcuTL7easx8A03AAAAFGDgBgAAgAIM3AAAAFCAgRsAAAAKMHADAABAAQZuAAAAKEAs2EGmKm4migzq06dP2LN69ersOIwoCmfJkiVhz6uvvpoVI3AorPhvKqrOySiuKor+qrnpppuyYmCqIlUaEtsURafs2rUr7Kmvr8++jrZs2ZJ1Tda89dZbyfqbb76Z/XMau6p/wyhOqCqS6oMPPsiOQdmfkWtVv0/0c6I4o6rYu+hvEx2rKv7rsssuyz5XH3nkkbCn6nrh0BWde6effnrY8/Wvfz1ZP/nkk7Ov1fXr1yfrs2bNCntmzpyZrG/YsCHsOeaYY5L1U089NewZOnRodtzrnDlzknWxYI1H9HnQr1+/sOfP/uzPkvVJkyZlzQxV50rV51QUoRmd9zXnnXdesv70009nx4I1dr7hBgAAgAIM3AAAAFCAgRsAAAAKMHADAABAAQZuAAAAKMCW8oNM1abNbt26ZW2lrWnXrl2y3rdv37An2lL4/PPPhz3z5s3L3vzMwSX6d482kVdtI//iF78Y9px11lnJeufOnbO32Udbj7dt2xb2RFtm16xZE/asWrUqq16zbNmyZP3dd9/N3vwcbaX9qN/1UBVtHX7wwQfDnuhvGG1erdm+fXuyvnHjxrBn69at2dtfe/bsmXV9nXHGGeGxhgwZkr2xNvrcibaX15x//vnJ+rPPPhv2RBv6JVY0fs2bx9/fjBo1Klm/9dZbw55og3l0ri5fvjw81n/9139l1Wtmz56dnbwyevTorE3kVSkzgwYNatDfmsYheja/5JJLwp4LL7wwWW/fvn12Akf0HBR9flU9C1Z9tkRb1ztU9ByqXLUAAABQgIEbAAAACjBwAwAAQAEGbgAAACjAwA0AAAAFGLgBAACgALFgjSiCKYpoOeqoo7KjNaIoipr3338/WX/iiSeyIwbEvTQeUbzcscceG/Zcf/31yfrZZ58d9kQxTFGE3KZNm8JjLVmyJFmfMWNG2PP6669nxcBU/ZwonqoqUqq+vj7siV6ritc7VK+xqt9ry5Ytyfrbb78d9syfPz/7PezPf4+qKJ8o1iV6z23atAmP1aNHj6wYmqr3VhU5GcWPde/ePeyJovcO1XO4KT27VEWN3njjjcn6uHHjwp7o3Fu8eHGyPmXKlPBY0bPLrFmzss/V3bt3hz3Rebx27dqwZ+DAgdnXUVXEIAePqnv+8OHDs6K/qqKBo/ivpUuXhseKPiur4lFHjhyZrI8YMSLs6dq1a7LevAlG2zW93xgAAAA+BgZuAAAAKMDADQAAAAUYuAEAAKAAAzcAAAAUYEv5AdKiRYtkvWfPnmHPueeem6xPmjQp7Im2B1Zt2vzVr36VrL/55pthT9XmZQ7+866mX79+yfoVV1wR9pxzzjn7bcNqtHm6arv0b3/722R92rRpYU90vKqN4zt27NhvG2vZd9HfPfp3qtm5c+d++/lVW4Kj11q2jD9uO3XqlLXJtiqZomob+f4UbZGOfpca25UbvyhJ5aKLLgp7/uiP/ijrWFXbyB988MFk/de//nV4rHnz5mUnYDTkfhGltVQ9H0Wfya1bt87++Rxcqu7FUYLQMccck/1z1q1bl6zPnDkz7HnhhReS9ZUrV4Y9nTt3zk6yiRI16pvgzOAbbgAAACjAwA0AAAAFGLgBAACgAAM3AAAAFGDgBgAAgAIM3AAAAFCAWLDCohiUKDpl4sSJ4bEuu+yy7IiYKHLiqaeeCnseeuihrMiLGhFIjeO8a9++fdhz6qmnJuunnXZa2NOtW7fs+LFdu3ZlRTqtWbMmPNaiRYuS9RUrVoQ9mzdvzo6Bid4zB5eq+1D0WkMivqrO7ygKpk+fPmHP+eefn3XPj+LCqj5bmjdvnv23ach9vSoWLPq7ub4OLlXXRBQBdPXVV4c9UUxk1b09iv964IEHkvWFCxeGx9q2bVuyXldXlx09WPW3ia6xDh06ZF8T0Xuuem8cGNE5URXzG93Dq6LEtm/fnqwvXbo0WZ87d254rFmzZmVH5UXXS1XkZXRv3xw8hx3K84RvuAEAAKAAAzcAAAAUYOAGAACAAgzcAAAAUICBGwAAAAqwpbywNm3aZG2E/sIXvhAe65hjjsnemPv2228n63fddVfY88477yTrNsk2HtHWyN69e4c9I0aMSNbbtm0b9tTX12fVq7Rq1SpZ79WrV9hz9NFHJ+vvvfde2LNu3brsrbA0TdHW4Y4dO4Y9Q4YMSdbHjx8f9lx44YXJ+siRI7N/fvR5ULVdOVK12fzwww9P1ocNGxb2vPbaa9kJAXz8qtIsovM42l5etd14ypQpYc+vfvWrZH3+/PlZG5yrnl2qtn03ZFNy9FlVta06em9LliwJe2wpP7hE99woxaVqG/nWrVvDnijJZd68ecn67Nmzw2MtXrw4+9qPnh+jZ7ealStXZicUHKp8ww0AAAAFGLgBAACgAAM3AAAAFGDgBgAAgAIM3AAAAFCALeX7QdWW8KFDhybrf/qnf5q1lbZqE2DVNsvvf//7yfqLL76YvQmxIVs7Kadqg3C0xbh///7Zm81XrFiRfe537do17Im2YEbnd9++fcNjjR07NutnVCUHvP7669mbzW3ub/yqtndH50rV5vzjjjsuWT/xxBPDnugcb9269WG5onOy6v4d3Uuq7jF9+vTJuiZrnnrqqezNvDYyf/znfpT+UDNp0qTse+7MmTOT9QcffDDsmTNnTlaaRNV5Er1WdU1Er0WfkzXHH3989v1i/fr12RumXRON4zqq2py/aNGiZP2tt97Kfg757//+72T9f/7nf8JjrV27Nvvaj+aZKs8//3zW73Io8w03AAAAFGDgBgAAgAIM3AAAAFCAgRsAAAAKMHADAABAAQZuAAAAKEAsWIYoIqV3795hz5e//OVk/ayzzsqKoanZtGlTsv7Tn/407HnssceS9S1btoQ94r8aR+REVQzLwIEDk/UBAwZkn19V0RILFizIjh+L3kOPHj2yr4lBgwYl6926dcuOYKqKMnv22WeT9TVr1oQ9IsMafyxYFMvVoUOHsCc6X6siYqKIlih2ryquq66u7rBc0bnfqVOnsKddu3bJ+kknnZQdm/TBBx+EPSKQyonO1ej5pGbUqFHZ0W5TpkxJ1l977bWwJzre/oz4quqJ7gtVny3jxo3LulaqPlvmz58f9nhGO7hE5+TKlSvDnmnTpiXrixcv3m8RcqtWrQqPFX2GTZw4Mew54ogjkvXNmzeHPY8++mhW/PChzDfcAAAAUICBGwAAAAowcAMAAEABBm4AAAAowMANAAAABdhSnrH9NdqifMMNN4Q9l156adb21507d2Zv+/u3f/u3sCfaomzza+PRsmXLrA3dNeeee252T7RpcunSpWHPe++9l6wvX748exv68OHDszZj1nTs2DF7i3S0kblLly7ZG2uffvrpsGfdunXJumuv8Yg2zUfbYqs2xm7bti3smTt3brLeuXPn7I3n0fXVq1evsGf8+PFZ12TVBvfoc7Lm5JNPztrY29Ct6+yd6N56wQUXhD1ROsZbb70V9jz55JNZ98iGbCNvyJbyhnzunn322WHP2LFjsze4P/LII1nJBTSez4kNGzaEPdOnT0/W33nnnexrIkodqppnPvGJTyTrEyZMyL4mnnnmmbAnSiLY0wQ37fuGGwAAAAowcAMAAEABBm4AAAAowMANAAAABRi4AQAAoAADNwAAABTQZGPBonX5VZEm1113XbJ+/fXXhz09e/bMihF44YUXwmN973vfS9aXLFkS9oggavyiGJYo+qvmoosuyooZqlmxYkVW9FZVrEvVz4mOF8UMRfWq11q1apXdM2LEiLDnvPPOy4qAqopnqor+4+NXFU+yY8eOZH3VqlVhTxQFM3/+/LCnRYsWWe8t+vyoadOmTbI+atSosGfMmDHZPyeKyouiY6oi+aqu1+jnNMVYmYaoigY65phjkvXRo0eHPfX19dnPLnPmzMmOfMuN/2rI+VD1txk8eHCy/oUvfCHs6d69e3Z85LPPPpv1d6bxqLp/RlFx0WdO1b0wqkfncM2NN96YrPfr1y/sieIw77nnnrBn48aN4WtNjW+4AQAAoAADNwAAABRg4AYAAIACDNwAAABQgIEbAAAACjikt5RHm19revfunaxfdtllYc8NN9yQrPfp0yfsibYHzpw5M1m/4447wmPNmjUrWbfNsmluKY82C1dtn+3QoUPYE22nHDJkSNizffv27M3i0Zby6L21bds2PFa03Ti67qpUbUqOtq5X/T2rNuBy8KjabhxtmY3O+4/aMpsrOocacq6edNJJYc+wYcOyr73Itm3bwtfee++97M39tpHvm6qt8ccdd1x2ykS0hf/555/P7qn6t/04tpH36tUr7PnKV76SrI8dOzY7teP+++8Pe5YvXx6+xqGrIQkU0UwTpSvdcsst4bHOOeecrPdV85Of/CQ7ocD9+//niRAAAAAKMHADAABAAQZuAAAAKMDADQAAAAUYuAEAAKAAAzcAAAAUcEjEgkURKQMHDgx7Jk2alKxfeeWV2bFJVbEbS5YsSda/853vJOsvv/xyeKyq6BQOXVUxEZE2bdok6+3atcuOH+vWrdt+jXyIIrty61Wq3lcU2xRFutS8++67yfqaNWv2678bH7+q8yuKE6q650fRLVU/J+qJIvSi2L+aK664IlmfMGFC2HPEEUdk/57R51EUX1kzderU7Cgx9k3Vv+FRRx2VHam6YsWKZH327NlhTxRduj8/P6piGPv27Zus33rrrWHP9ddfn6zv3r077LnvvvuS9aeeeirsEevK3p7HUVxfFGH3uc99LvsZ8bnnngt7/vEf/zFZd//eO77hBgAAgAIM3AAAAFCAgRsAAAAKMHADAABAAQZuAAAAaMpbyqNN5DUjRoxI1q+55pqwZ+zYscn60KFDs7f6bd68Oey59957k/UnnngiWd++fXt4LJqm6Pyq2mh/zjnnJOsdOnTIvsYasiW8IaKNtVWbbKNN4NEm8prly5cn66+88krYE23uXLlyZfZ748CIzuOqLc49evTITsCINiJXbfvv3r171jbyk046KTzW4MGDs6/9aDNu1Tn83nvvJet333132DNnzpzsn8O+qdo4Hm09rrrnR/9WVdu292cCRevWrZP14cOHhz1/+Zd/mZVWU7WN/IEHHgh7fvjDHybrGzduDHs4dFWd39HnTpcuXcKez3/+88n6DTfckHV918ydOzd7c//SpUvD1/hovuEGAACAAgzcAAAAUICBGwAAAAowcAMAAEABBm4AAAAowMANAAAATSEWLFqj37t377Dni1/8YrL+h3/4h9kxLG3bts2Ow3j88cfDnsmTJyfrGzZsyI5AommKouKmTJkS9gwYMCBZ/+xnP5sdZ1R1TTQk1iWKW4liZaoivqK4lffffz/sieLUpk6dGvbMmDEjWd+0aVPY41o+uETnaseOHcOe8ePHJ+tXXnll2BPFE7Vv3z476iiKooz++4+Kgcr9bFu8eHHY80//9E/Z96WdO3dmvzf2TXS/rbp/Vt3X+/XrlxW1WvUZVldXF/YcfvjhyfqECROyIpNqjj766GR9y5YtYc/999+frH/7298Oez744INk3WfBoS26Xqru09H5XfXZcvPNN2cdqyq29Otf/3qyPn369LDHebxvfMMNAAAABRi4AQAAoAADNwAAABRg4AYAAIACDNwAAADQFLaUN2+e/n8ARx11VNhz+umnJ+u9evUKe1q1apW9RfXZZ59N1r/zne+EPdG25KrNobA358rSpUvDnu9973vJ+vPPPx/2TJw4MVk/7rjjwp6uXbselmvr1q3J+tq1a7M2v9bMmzcvWZ81a1bYs2DBgmR9zZo1Yc+2bduytjvX2OjZOHTq1Cl87ZxzzknWTznllLCnS5cuWZ9tDd32n3veVW37f+edd5L1H/3oR2HPL37xi+zNz3z8qp5posSGqjSLKOHlb/7mb7KTZKqeg6LNy9FzXfRMV7NkyZKsTftVCTOrV68Oe9zzm6bo3Bs8eHDYc/311yfrV111VdjTp0+frLSUO+64IzzWU089lZUWw77zDTcAAAAUYOAGAACAAgzcAAAAUICBGwAAAAowcAMAAEABBm4AAABoCrFgUazC+vXrw57FixdnrdCvikh55plnwp577rknWZ85c2bYU1dXF74G+6IqUiW6XqrO72nTpmXHrbRs2TI7Aim6xqOIrarorehvUPW3iV6rinQR93LoqorLWr58eVa0XVXMWEOiv6LzrupzZdWqVcn6Sy+9FPbcf//92TGCmzdvDl/j4FEV8/P0008n6w888EDYc80112TFhX3Ua5Hovr9x48Zk/bnnnst+dnvxxRfDHuc3e/tMM2DAgGT9q1/9athz6aWXJuvdunXLPiejGNif//zn2VGnlOMbbgAAACjAwA0AAAAFGLgBAACgAAM3AAAAFGDgBgAAgAKa7dnL9bsN2bD6cdmf78024qZjX/+tD+ZrAhqiKV4TrVu3Dl876qijkvWrr7467Bk/fnyy3rNnz+z3FqUNvPXWW2HP448/nr3FefXq1dkbrpuKpnhNVCVTDBkyJHsj86mnnpq9KTnaIP7QQw9lp8VE25097zVMU7wm2rVrF7527bXXJuvf/OY3w54oRWnLli1hz49//ONk/bvf/W6yvmLFivBYzv39a2/+nr7hBgAAgAIM3AAAAFCAgRsAAAAKMHADAABAAQZuAAAAKMDADQAAAAUcErFg0BBNMdoCqrgm4MNcE/BhTfGaaNOmTfjaddddl6zffvvt2dF7P/zhD8Oeu+++Oys+ko+PWDAAAAA4QAzcAAAAUICBGwAAAAowcAMAAEABBm4AAAAowJZymqymuGkTqrgm4MNcE/BhrokPa9u2bbLes2fPsGfDhg3J+ubNm4v93SnHlnIAAAA4QAzcAAAAUICBGwAAAAowcAMAAEABBm4AAAAowMANAAAABYgFo8kSbQEf5pqAD3NNwIe5JuDDxIIBAADAAWLgBgAAgAIM3AAAAFCAgRsAAAAKMHADAADAgdxSDgAAAOw933ADAABAAQZuAAAAKMDADQAAAAUYuAEAAKAAAzcAAAAUYOAGAACAAgzcAAAAUEDLvf0PmzVrVuLnwwGzrxH0rgkONa4J+DDXBHyYawLyrwnfcAMAAEABBm4AAAAowMANAAAABRi4AQAAoAADNwAAABRg4AYAAIACDNwAAABQgIEbAAAACjBwAwAAQAEGbgAAACjAwA0AAAAFGLgBAACgAAM3AAAAFGDgBgAAgAIM3AAAAFCAgRsAAAAKMHADAABAAQZuAAAAKKBliYMCAAePZs2aha+1atUqq15TV1eXVa/Zs2dP5XsEoLzmzZtn36Pdv/eNb7gBAACgAAM3AAAAFGDgBgAAgAIM3AAAAFCAgRsAAAAKMHADAABAAWLBCotiVQYOHJisX3jhheGxTjjhhGS9vr4+7JkxY0ayPm3atLBn4cKFyfq2bdvCHnEBNEVVUUtR7EZVz+7du7Pq8H917tw5WZ80aVLYc8MNNyTrvXr1CnveeuutZP3uu+8Oe15++eVkfceOHWEPlNKyZfwI3LZt2+znoF27du2X9wX7Y84YO3Zs2PPtb387+1njjjvuSNaffPLJsKfqemlqfMMNAAAABRi4AQAAoAADNwAAABRg4AYAAIACDNwAAABQgC3l+0G0zbLm3HPPTda/+c1vJusjRowIjxVtD1y3bl3Y069fv+ztnNEG8/nz54c9W7duTdZtL+dg06JFi2R98ODB2VucP/nJT4Y93bt3T9Y3btwY9jz22GPJ+j333BP2rFixInyNQ1ObNm3C16LNtLfddlvYM2jQoOz3EG3hP+6447JTM3bu3Bn2+AxhX7Vv3z5Zv/POO8OeKDHm7//+78OeyZMnJ+vbt2//yPcIDRXdi3v06BH29O/fP+tYDZ0nolSWPU3wvu4bbgAAACjAwA0AAAAFGLgBAACgAAM3AAAAFGDgBgAAgAIM3AAAAFCAWLAM0br8YcOGhT1//ud/nqyPGTMmWV+9enV4rOnTp2fHDHXt2jVZP/7448Oe999/P1lftmxZ2BPFXuzatSvsgVKi877m85//fLL+x3/8x2HPwIEDsyMBcyP0ak499dRk/ZFHHgl7Vq1alRUjSOPRkOitKHJy6NChYU8Uy7Vly5awp3Xr1sn6ySefHPY8+eST2Z9hTTE+hv1r5MiRyfoVV1yRHb1X9UzjXOVAiM67KAK1plWrVlnHqtm8eXOyXl9fn/3emiLfcAMAAEABBm4AAAAowMANAAAABRi4AQAAoAADNwAAABRgS3mGaGvl2WefHfYMHz48WV+4cGGy/g//8A/hsd54441kvVevXmHPpZdemqwPGDAg7Onbt2+y3q5du7CnWbNm4WuwL6rOrSOOOCJZ/+u//uuw5zOf+Uz2ZvOqbZ+RhmwJX7duXbJui3PTFN2nb7/99rDn2GOPzd44/tJLLyXrS5YsCXuipItoI3TVdvVFixaFPbbtszeq7tGXXHJJ9j3/gw8+SNZnzJgR9tTV1VW+R/g49enTJ3tL+aZNm8KexYsXJ+vO+73jG24AAAAowMANAAAABRi4AQAAoAADNwAAABRg4AYAAIACDNwAAABQgFiwDF26dMmKR6mpr69P1p944olk/be//W14rCgaKIorq+nZs2dWnFLV7wkHIv6rKvbub//2b5P1iy++OOzp1KlTst68ef7/f6yK5Nq5c2dW3EzNlClTkvVly5Y16D3QOHTr1i1Zv+mmm5L1cePGhcfavn171rlV87Of/SxZ37x5c9hzzTXXJOsTJkwIe6IIzarPPZEz7I3ovl4zfvz47Hv+iy++mKwvWLAg7BFhx4EQRXydcMIJ2T2rV6/OjgXbtWvXR75HfMMNAAAARRi4AQAAoAADNwAAABRg4AYAAIACDNwAAABQgC3le7kpuaZjx47JeteuXcOeLVu2JOuLFi3K2jBbtY28ahPh6NGjsze/Rptpt23bFvbYzsneijbD9u7dO1n/xje+ER5r0qRJWdfqR13juZvAo03kNcuXL0/Wf/nLX4Y9UXpB1X2BxqF9+/bha5dffnmyfu2112b/nGnTpiXrP/7xj8Oet99+O2uTbdVnWLt27cKeo48+OrsnSueA/2348OHha8OGDct+DnryySeT9U2bNjXg3cHBlaAUPYfNmTMn7Fm1alWyLill7/iGGwAAAAowcAMAAEABBm4AAAAowMANAAAABRi4AQAAoAADNwAAABQgFmw/RAZVxWXV19cn6z179kzWjzjiiPBY/fv3T9avueaasKdXr17J+ksvvRT2TJ8+PTueRSwA/1vLlvGtZcCAAcn6Lbfckqx/5jOfCY/VqVOn7Os4OlerzuFdu3ZlXxNPP/10sn7fffeFPWvXrs1+bzSOc/+8884Le26++eZkvXPnzsn673//+/BY//qv/5qsv/nmm9mfYW3btj0sV+vWrcPXGnI82Js4o/Hjx4c9UUzk0qVLw55XXnkl67MASouea4YMGZL1rFX1TFH12RJFBrN3fMMNAAAABRi4AQAAoAADNwAAABRg4AYAAIACDNwAAABQgC3lGaINfe+++27Yc9xxxyXrp5xySvYW1xNPPDHrZ9SsXr06WX/iiSfCnpkzZybrO3bsCHtsUW6aWrVqlayPGDEi7LnpppuS9QsuuCBZ79atW/bG2t27dx+Wq2qzeXS8FStWhD0PPfRQsr5s2bKwx3XUOFSdK8cee2yy/pWvfCXsGTp0aLK+ePHiZH3y5MnZ25W3bt2afX5X/Z5RAkZ0T6ja6r9z586wB/bmGWncuHHZnxOvvfZa2LNkyZJk3T2aAyW6H5988snJeocOHbKTKd54442wp66u7iPfIzHfcAMAAEABBm4AAAAowMANAAAABRi4AQAAoAADNwAAABRg4AYAAIACxIJlRD5EkSavvvpq2HPSSScl66NGjUrWR44cGR6ra9eu2XEvjz76aLL+yCOPhD3r1q3bb1FLNH4tW8a3iWHDhiXrN998c9gzceLEZL1nz57JeosWLcJj7dq1K6te9ftU/ZxIVcTX7Nmzs98bjUOPHj3C16699tpk/eyzzw57tm/fnqz/+te/zo51jD6nGnL/7tixY/ja4MGDs6PEZs2alRVRA//XkUcemRXHV3XuT506NexxTnKwiZ5dzjjjjOxnmlWrViXr8+bNC3tE4u0b33ADAABAAQZuAAAAKMDADQAAAAUYuAEAAKAAAzcAAAAUYEt5xha+nTt3JusLFiwIexYuXJisn3nmmdlbYXfs2JGsP/fcc2HP5MmTk/UPPvgg7LFFuWlq3jz9/9/69OkT9vzJn/xJ1ibyqm3k0QbOqvMx2u4cXatV11jVNvbovlB17a9fvz7rWBx8onNi3LhxYc+VV16ZrLdq1SrseeGFF5L1X/ziF8n6ypUrszcyV5130TbbESNGhD1RokZ0Tda88sor2dcrTVO07X7MmDHJerdu3cJjbdmyJet8rPEcxMEmenaJrokqc+fOzdpezr7zDTcAAAAUYOAGAACAAgzcAAAAUICBGwAAAAowcAMAAEABtpRniLa8Vm2zjDZttmnTJntT8saNG5P11157LexZvHhxsl5fXx/20DS1b98+Wb/00kvDnkmTJmVtIq86x6Ptylu3bg2PtWLFimS9devWYU+nTp2yrtWqzcuvvvpqdg+NR+/evZP1q6++Ouw5/PDDs+7FNT/72c+yNslW3b+jz6mq87tr167ZaQNRekHVNTFjxoysa5+mK0rNOPnkk7OfnZYsWZKsv//++w18d/DxO/roo5P1vn37ZidTRHOD55ZyfMMNAAAABRi4AQAAoAADNwAAABRg4AYAAIACDNwAAABQgIEbAAAAChALliGKnRg0aFDYM3r06Kxj1dXVZcdkHHXUUWFPFI9UFRFTFSVA41YVnXL66adnRyBF0UAtWrQIe6JIo9WrVyfrc+bMCY8VRViMGTMm+29Qdd6/++67yfrvfve7sEf0XuNQda6ecsopyfqZZ54Z9kT38MceeyzseeaZZ7LO76pzNbq3t23bNuz51Kc+lax/7nOfC3uiOMzJkydnx/jB3t6njz/++OxjRfF6mzdvzj4WlFT1bD527Nise/u2bdvCY7300kvZMcfsG99wAwAAQAEGbgAAACjAwA0AAAAFGLgBAACgAAM3AAAAFGBLecaGwJ49eybrEydODHuGDx+erG/YsCFZX7NmTfbPHz9+fNjz8MMPJ+srV64Me3bv3h2+RuM+j/v16xf23Hjjjcn64MGDszfnV23HXLRoUbL+7LPPJuubNm0KjzVhwoRkvXv37tlbqas21v7Hf/xH1u9SY9t/49C1a9fwtYsvvjhZ79SpU9gze/bsZP3nP/952BPd9xtyL27Tpk2y/slPfjLs+epXv5qs9+rVK+x59NFHk/WpU6eGPVUpHPC/tW/fPjsVJndLufORg02ULFRz3nnnZT3vLV26NDzWvHnzknXPLeX4hhsAAAAKMHADAABAAQZuAAAAKMDADQAAAAUYuAEAAKAAAzcAAAAUIBYsYyX/Kaeckh0LFvnNb36TrE+fPj3sufLKK5P1kSNHhj0XXHBBsv7qq6+GPevWrQtfo3Ho3Llzsn7VVVeFPaNGjcqOiVi/fn1WDEtV/Ffk/PPPD1+Lzv22bduGPTt27EjWn3rqqbDnpz/9adaxOPhEEXZRdGPN2Wefnazv3LkzOy4rigurqa+vP2x/RH9VxX/dfvvtYc+xxx6brC9YsCDs+cEPfpCsr1ixIuwROcPe6tKlS1Y8alWE3vz585N15yMHm969e4evVT3rp7zzzjvha2vXrk3WXRPl+IYbAAAACjBwAwAAQAEGbgAAACjAwA0AAAAFGLgBAACggCa7pbxZs2bJerdu3cKeCRMmJOv9+vULe2bMmJGs33PPPcn6ypUrw2P1798/e3Nh9Fq0AbRq87TthQeXqk3FZ5xxRrJ+6qmnhj3Rxu3FixeHPUuXLk3WV61aFfacdtppWZuSq7Z2tmyZvoVt2rQp7Hn88ceT9W984xthz/Lly8PXaNwJFGeddVbYE21ErjofovNr27Zt2Z9H7dq1y97c/7WvfS1ZHzNmTNgT/T5Vm81ff/31ZH3Xrl1hD+ytHj16ZCVQVG36f++995J1zzQcKNE9/xOf+ET2NRFt6I+2839U0gZl+IYbAAAACjBwAwAAQAEGbgAAACjAwA0AAAAFGLgBAACgAAM3AAAAFNBkY8GaN0//v4aBAwdmxxlVxVH88pe/TNZnz559WK4onqkq2iKK0IjqNJ5ztSqOLor/qorsWbhwYXZPFNk1evTo7GiLVq1a7bf3/C//8i9hz7333pusr1u3LuwRH9P4de7cOTsqr0WLFsn63Llzw55ly5ZlHaume/fuyfrll1+erH/5y18OjxV9hlXF+/3FX/xFsj5lypTsGEHY12ikqus1+pyoit1bsWJFsu6+zoESncfnnXdedrRlXV1dsr5o0aLwWFGUWNU16XrZN77hBgAAgAIM3AAAAFCAgRsAAAAKMHADAABAAQZuAAAAKKDJbilv2TL9qw8aNCjsOeKII5L1LVu2hD1z5szJ2vbXtWvX8FhnnXVWst6mTZuwZ+vWrVlbDTn4tG/fPlk/++yzw55Ro0ZlbX6t2oDZv3//sKdXr177bQt+dB1NmzYt7LnzzjuT9ddeey3s2blzZ/Z7o/Hr2LFjsn7kkUeGPdHG1mjDbNW1V9UTbSOfOHFist6hQ4fsz5xbb7017ImuMdcKB0qnTp2yrsmNGzeGx6p6DQ6EKK2lKjUjOve3b9+erK9evTo7/YZy/MUBAACgAAM3AAAAFGDgBgAAgAIM3AAAAFCAgRsAAAAKMHADAABAAU02FixaiV8VsRVFibVr1y7sufTSS7OiaC644ILwWJ/+9KezIsZqZs6cmayvW7cu7Kk6HmVEcQ9VUXUXX3xx2HPCCSdkRYxVncdVEV/RdVR1Dq1cuTJZv/fee5P1u+66KzzW8uXLk/Xdu3eHPTRN0TlRFZHYokWLZP20004Le37yk59kf7ZEcX3RdfTyyy+Hx7rtttuS9VdffTXsERPJwfa5F8WCRVatWhW+FsUmQUlV0VtDhw5N1gcMGLDfYsE2b96cfSzK8Q03AAAAFGDgBgAAgAIM3AAAAFCAgRsAAAAKMHADAABAAU12S3l9fX2y/v7772dvVx45cmTY86UvfSlZv/7667M32UYba1966aWw57HHHkvWN23aFPZwcG2zHDVqVLI+evTosKd3795Zm/Y/6j1Edu3alawvWLAg7Pm7v/u7ZP3BBx/M3rQJeytKZqi6f44ZMyZZ79ixY9gTvVa1uT+6Hz/88MPJ+ne/+93wWHPmzMn6zIMDpeozp1evXlnblZcuXRoeKzr3qzY1S2uh5PkdPaO1atUqO2lj/fr1WfWqZzfK8Q03AAAAFGDgBgAAgAIM3AAAAFCAgRsAAAAKMHADAABAAQZuAAAAKEAs2P8xffr0sOfOO+9M1v/qr/4q7Onfv3+y3rp16+wIpJdffjlZv+uuu8Ke119/PVkXEdN4RPENVZEmURxFQ2JQtm3bFva8+OKLyfodd9wR9kQxTDt37gx7YF9t3LgxWf/+978f9tTV1SXrl112WdgTRTvOnTs37Pn3f//3ZP3xxx9P1teuXRseS5wRjUXV51GnTp32W6Rr1ONaoaQoxqtm/vz5WfWawYMHZz2HVcWzRs9brolyfMMNAAAABRi4AQAAoAADNwAAABRg4AYAAIACDNwAAABQQLM9e7mSrmqbJPl/t4b8PaN/KlsFG2Zf/27785qoOlbPnj2T9VtuuSXsueSSS7I2v9asXLkyWX/ooYfCnvvuuy9ZX7p0aYM2d3JgHUzXBBwMXBPlRGkaNRdddFGy/q1vfSs7beCBBx5I1nfs2PGR75H/l2sC8q8J33ADAABAAQZuAAAAKMDADQAAAAUYuAEAAKAAAzcAAAAUYOAGAACAAsSC0WSJtoAPc03Ah7km4MNcE/BhYsEAAADgADFwAwAAQAEGbgAAACjAwA0AAAAFGLgBAACgAAM3AAAAFGDgBgAAgAIM3AAAAFCAgRsAAAAKMHADAABAAQZuAAAAKMDADQAAAAUYuAEAAKAAAzcAAAAUYOAGAACAAgzcAAAAUICBGwAAAAowcAMAAEABzfbs2bOnxIEBAACgKfMNNwAAABRg4AYAAIACDNwAAABQgIEbAAAACjBwAwAAQAEGbgAAACjAwA0AAAAFGLgBAACgAAM3AAAAHLb//X+lcCwx0wFPeAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Samples\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_samples = new_model_worker.generate_samples(10, 8)\n",
    "prev_samples = model_worker.generate_samples(10, 8)\n",
    "\n",
    "print(\"Old Samples\")\n",
    "plot_generated(prev_samples)\n",
    "\n",
    "print(\"New Samples\")\n",
    "plot_generated(new_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3f164629-dd2a-4049-9346-a23f732920ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SimpleCNNFlowModel(nn.Module):\n",
    "#     def __init__(self, latent_dim=3, n_hidden=32, n_layers=3, act=nn.LeakyReLU):\n",
    "#         super(SimpleCNNFlowModel, self).__init__()\n",
    "#         self.latent_dim = latent_dim\n",
    "# Later try to do spatial        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f57ca3a6e504913f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T03:47:37.068248Z",
     "start_time": "2025-10-07T03:47:37.065744Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleFlowModel(nn.Module):\n",
    "    def __init__(self, latent_dim=3, n_hidden=32, n_layers=3, act=nn.LeakyReLU):\n",
    "        super(SimpleFlowModel, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(latent_dim+1, n_hidden), act(),\n",
    "            *[nn.Sequential(nn.Linear(n_hidden, n_hidden), act()) for _ in range(n_layers-1)],\n",
    "            nn.Linear(n_hidden, latent_dim),)\n",
    "\n",
    "    def forward(self, x, t, act=F.gelu):\n",
    "        t = t.expand(x.size(0), 1)  # Ensure t has the correct dimensions\n",
    "        x = torch.cat([x, t], dim=1)\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f48e75a340627d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "add6f641-e918-4317-8fd7-339c9820caeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "from matplotlib import rc\n",
    "from tqdm import tqdm\n",
    "\n",
    "@torch.no_grad()\n",
    "def create_latent_flow_animation(\n",
    "    flow_model,\n",
    "    latent_dataloader,  # Dataloader with (latents, labels) where latents is [B, D]\n",
    "    n_samples=500,\n",
    "    n_frames=50,\n",
    "    model2=None,  # Optional second flow model for comparison\n",
    "    titles=None,\n",
    "    save_file=None,\n",
    "    figsize=(5, 5),\n",
    "    device='mps',\n",
    "    reducer='tsne',  # 'pca' or 'tsne'\n",
    "    perplexity=30\n",
    "):\n",
    "    \"\"\"\n",
    "    Create animation showing how latents flow through your flow model.\n",
    "    Shows initial noise distribution flowing to target latent distribution.\n",
    "    Uses consistent 2D projection (fit once, apply to all trajectories).\n",
    "    \n",
    "    Args:\n",
    "        flow_model: Your trained flow model (takes x, t)\n",
    "        latent_dataloader: DataLoader yielding (latents, labels) \n",
    "                          where latents shape is [B, D] (e.g., [B, 7])\n",
    "        n_samples: Number of points to animate\n",
    "        n_frames: Number of animation frames\n",
    "        model2: Optional second flow model for comparison\n",
    "        save_file: If provided, saves animation as MP4\n",
    "        reducer: 'pca' or 'tsne' for dimensionality reduction\n",
    "        perplexity: Perplexity for t-SNE (ignored if reducer='pca')\n",
    "    \"\"\"\n",
    "    flow_model.eval()\n",
    "    if model2 is not None:\n",
    "        model2.eval()\n",
    "    \n",
    "    # 1) Extract latents from dataloader\n",
    "    print(\"Loading latents from dataloader...\")\n",
    "    latents, labels = [], []\n",
    "    for z_batch, y_batch in latent_dataloader:\n",
    "        if len(latents) * z_batch.size(0) >= n_samples:\n",
    "            remaining = n_samples - sum(z.size(0) for z in latents)\n",
    "            if remaining <= 0:\n",
    "                break\n",
    "            z_batch = z_batch[:remaining]\n",
    "            y_batch = y_batch[:remaining]\n",
    "        \n",
    "        latents.append(z_batch)\n",
    "        labels.append(y_batch)\n",
    "    \n",
    "    latents = torch.cat(latents, 0).to(device)\n",
    "    labels = torch.cat(labels, 0).cpu().numpy()\n",
    "    \n",
    "    print(f\"Got {len(latents)} latent points with dimension {latents.shape[1]}\")\n",
    "    \n",
    "    latent_dim = latents.shape[1]\n",
    "    \n",
    "    # 2) Generate trajectories in full latent space FIRST\n",
    "    print(\"Computing trajectories with RK4...\")\n",
    "    \n",
    "    # Start from noise in full latent space\n",
    "    noise_full = torch.randn_like(latents)\n",
    "    noise_full = noise_full * latents.std(dim=0, keepdim=True) + latents.mean(dim=0, keepdim=True)\n",
    "    \n",
    "    _, trajectories_full = integrate_path(\n",
    "        flow_model, noise_full, \n",
    "        step_fn=rk4_step, \n",
    "        n_steps=n_frames,\n",
    "        save_trajectories=True,\n",
    "        warp_fn=warp_time\n",
    "    )\n",
    "    \n",
    "    trajectories_full2 = None\n",
    "    if model2 is not None:\n",
    "        print(\"Computing trajectories for model 2...\")\n",
    "        _, trajectories_full2 = integrate_path(\n",
    "            model2, noise_full,\n",
    "            step_fn=rk4_step,\n",
    "            n_steps=n_frames, \n",
    "            save_trajectories=True,\n",
    "            warp_fn=warp_time\n",
    "        )\n",
    "    \n",
    "    # 3) Fit reducer on ALL data (target latents + all trajectory points) for consistency\n",
    "    if latent_dim > 2:\n",
    "        print(f\"Fitting {reducer.upper()} on all data for consistent projection...\")\n",
    "        \n",
    "        # Combine target latents + all trajectory frames for fitting\n",
    "        all_data = [latents.cpu().numpy()]\n",
    "        for i in range(len(trajectories_full)):\n",
    "            all_data.append(trajectories_full[i].cpu().numpy())\n",
    "        if trajectories_full2 is not None:\n",
    "            for i in range(len(trajectories_full2)):\n",
    "                all_data.append(trajectories_full2[i].cpu().numpy())\n",
    "        \n",
    "        all_data = np.vstack(all_data)\n",
    "        print(f\"Fitting on {len(all_data)} total points...\")\n",
    "        \n",
    "        if reducer.lower() == 'tsne':\n",
    "            from sklearn.manifold import TSNE\n",
    "            perplexity = min(perplexity, len(all_data) // 4)\n",
    "            projection = TSNE(\n",
    "                n_components=2, \n",
    "                perplexity=perplexity,\n",
    "                learning_rate=200,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            all_data_2d = projection.fit_transform(all_data)\n",
    "            print(\"t-SNE fitting complete\")\n",
    "        else:\n",
    "            from sklearn.decomposition import PCA\n",
    "            projection = PCA(n_components=2)\n",
    "            all_data_2d = projection.fit_transform(all_data)\n",
    "            print(f\"PCA explained variance: {projection.explained_variance_ratio_.sum():.2%}\")\n",
    "        \n",
    "        # Split back into original components\n",
    "        n_latents = len(latents)\n",
    "        n_per_frame = len(trajectories_full[0])\n",
    "        \n",
    "        latents_2d = torch.tensor(all_data_2d[:n_latents], dtype=torch.float32, device=device)\n",
    "        \n",
    "        trajectories = []\n",
    "        offset = n_latents\n",
    "        for i in range(len(trajectories_full)):\n",
    "            traj_2d = all_data_2d[offset:offset+n_per_frame]\n",
    "            trajectories.append(torch.tensor(traj_2d, dtype=torch.float32, device=device))\n",
    "            offset += n_per_frame\n",
    "        \n",
    "        trajectories2 = None\n",
    "        if trajectories_full2 is not None:\n",
    "            trajectories2 = []\n",
    "            for i in range(len(trajectories_full2)):\n",
    "                traj_2d = all_data_2d[offset:offset+n_per_frame]\n",
    "                trajectories2.append(torch.tensor(traj_2d, dtype=torch.float32, device=device))\n",
    "                offset += n_per_frame\n",
    "        \n",
    "        # Get noise_2d for plot range\n",
    "        noise_2d = trajectories[0]\n",
    "        \n",
    "    else:\n",
    "        # Already 2D, no projection needed\n",
    "        latents_2d = latents\n",
    "        noise_2d = noise_full.to(device)\n",
    "        trajectories = [trajectories_full[i].to(device) for i in range(len(trajectories_full))]\n",
    "        trajectories2 = None\n",
    "        if trajectories_full2 is not None:\n",
    "            trajectories2 = [trajectories_full2[i].to(device) for i in range(len(trajectories_full2))]\n",
    "    \n",
    "    # 4) Setup plot\n",
    "    if titles is None:\n",
    "        titles = ['Flow Model']\n",
    "        if model2:\n",
    "            titles.append('Model 2')\n",
    "    \n",
    "    n_plots = 1 + (model2 is not None)\n",
    "    if model2:\n",
    "        figsize = (figsize[0] * 2, figsize[1])\n",
    "    \n",
    "    fig, axes = plt.subplots(1, n_plots, figsize=figsize)\n",
    "    if n_plots == 1:\n",
    "        axes = [axes]\n",
    "    plt.close()\n",
    "    \n",
    "    # Determine plot ranges (fixed across all frames)\n",
    "    all_points = torch.cat([noise_2d, latents_2d], dim=0).cpu().numpy()\n",
    "    max_range = max(abs(all_points).max(), 3.0)\n",
    "    \n",
    "    # Setup axes\n",
    "    for ax in axes:\n",
    "        ax.set_xlim(-max_range, max_range)\n",
    "        ax.set_ylim(-max_range, max_range)\n",
    "        ax.set_aspect('equal')\n",
    "    \n",
    "    # 5) Create animation\n",
    "    print(\"Generating animation frames...\")\n",
    "    \n",
    "    # Compute warped timesteps for display\n",
    "    ts = torch.linspace(0, 1, n_frames + 1)\n",
    "    ts_warped = warp_time(ts).numpy()\n",
    "    \n",
    "    # Progress bar for animation rendering\n",
    "    pbar = tqdm(total=n_frames, desc=\"Rendering frames\", unit=\"frame\")\n",
    "    \n",
    "    def init():\n",
    "        for ax in axes:\n",
    "            ax.clear()\n",
    "            ax.set_xlim(-max_range, max_range)\n",
    "            ax.set_ylim(-max_range, max_range)\n",
    "        return []\n",
    "    \n",
    "    def animate(frame):\n",
    "        pbar.update(1)  # Update progress bar\n",
    "        t_display = ts_warped[frame]\n",
    "        \n",
    "        for i, ax in enumerate(axes):\n",
    "            ax.clear()\n",
    "            ax.set_xlim(-max_range, max_range)\n",
    "            ax.set_ylim(-max_range, max_range)\n",
    "            ax.set_aspect('equal')\n",
    "            \n",
    "            reducer_label = reducer.upper() if latent_dim > 2 else \"\"\n",
    "            title = f\"{titles[i]} (t={t_display:.2f})\"\n",
    "            if reducer_label:\n",
    "                title += f\" [{reducer_label}]\"\n",
    "            ax.set_title(title)\n",
    "            \n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            for spine in ['top', 'right', 'bottom', 'left']:\n",
    "                ax.spines[spine].set_visible(False)\n",
    "            \n",
    "            # Plot target latents (faint, always visible)\n",
    "            ax.scatter(latents_2d[:, 0].cpu(), latents_2d[:, 1].cpu(), \n",
    "                      c='lightgray', alpha=0.3, s=8, label='Target', zorder=1)\n",
    "            \n",
    "            # Plot flowing particles (brighter, on top)\n",
    "            if i == 0:\n",
    "                current = trajectories[frame]\n",
    "                ax.scatter(current[:, 0].cpu(), current[:, 1].cpu(), \n",
    "                          c=labels, cmap='tab10', alpha=0.7, s=15, \n",
    "                          edgecolors='white', linewidth=0.3, zorder=2)\n",
    "            else:\n",
    "                current = trajectories2[frame]\n",
    "                ax.scatter(current[:, 0].cpu(), current[:, 1].cpu(), \n",
    "                          c=labels, cmap='tab10', alpha=0.7, s=15,\n",
    "                          edgecolors='white', linewidth=0.3, zorder=2)\n",
    "        \n",
    "        return []\n",
    "    \n",
    "    anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                                   frames=n_frames, interval=50, blit=False)\n",
    "    \n",
    "    if save_file:\n",
    "        print(f\"Saving animation to {save_file}...\")\n",
    "        anim.save(save_file, writer='ffmpeg', fps=20)\n",
    "        pbar.close()\n",
    "        print(\"Done!\")\n",
    "        return HTML(f\"\"\"<center><video height=\"400\" controls loop>\n",
    "                    <source src=\"{save_file}\" type=\"video/mp4\">\n",
    "                    Your browser does not support the video tag.</video></center>\"\"\")\n",
    "    else:\n",
    "        pbar.close()\n",
    "        rc('animation', html='jshtml')\n",
    "        return HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "aeaf6c29-a7ff-43f7-99dc-aee5faa0a002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading latents from dataloader...\n",
      "Got 512 latent points with dimension 7\n",
      "Computing trajectories with RK4...\n",
      "Computing trajectories for model 2...\n",
      "Fitting TSNE on all data for consistent projection...\n",
      "Fitting on 512512 total points...\n",
      "t-SNE fitting complete\n",
      "Generating animation frames...\n",
      "Saving animation to flow_comparison.mp4...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<center><video height=\"400\" controls loop>\n",
       "                    <source src=\"flow_comparison.mp4\" type=\"video/mp4\">\n",
       "                    Your browser does not support the video tag.</video></center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_latent_flow_animation(\n",
    "    flow_model=pretrained_flow_model,\n",
    "    latent_dataloader=train_dl,\n",
    "    model2=new_model,\n",
    "    n_samples=500,\n",
    "    n_frames=500,\n",
    "    titles=['Original Flow', 'Reflowed Model'],\n",
    "    save_file='flow_comparison.mp4',\n",
    "    device='mps'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b81c41-e3e7-4d02-b27b-577105bf868c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
