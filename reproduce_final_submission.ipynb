{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Reproduce Final Submission - Full Training Pipeline\n\n**Note:** This notebook was generated by Claude Code on behalf of Marco Cassar. The original training repository was provided to compile this notebook version for ease of access in Google Colab.\n\nThis notebook trains a VAE + Flow model from scratch following the exact pipeline:\n1. Train VAE on MNIST\n2. Save VAE with EMA weights\n3. Encode MNIST dataset using trained VAE\n4. Train flow model on encoded latents\n5. Save flow model with EMA weights\n6. Test generation",
   "id": "7c4f7de9b93542b2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ],
   "id": "80cc28ee05e55c9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install wandb torch_ema",
   "id": "d19a3c966120b81"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_ema import ExponentialMovingAverage\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm\n",
    "from safetensors.torch import save_file, load_file\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from math import log2\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ],
   "id": "9f724ab790530c38"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definitions"
   ],
   "id": "9574089a9e7a55d2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy all model classes from marco_submission_2.py\n",
    "# ResidualBlock, InspoResNetVAEEncoder, InspoResNetVAEDecoder, InspoResNetVAE, FlatVelocityNet\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, use_skip=True, use_bn=True, act=nn.GELU, dropout=0.4, groups=1):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1, bias=not use_bn, groups=groups)\n",
    "        self.bn1 = nn.BatchNorm2d(channels) if use_bn else nn.Identity()\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1, bias=not use_bn, groups=groups)\n",
    "        self.bn2 = nn.BatchNorm2d(channels) if use_bn else nn.Identity()\n",
    "        self.use_skip, self.act = use_skip, act\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_skip: x0 = x\n",
    "        out = self.act()(self.bn1(self.conv1(x)))\n",
    "        out = F.dropout(out, self.dropout, training=self.training)\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.use_skip: out = out + x0\n",
    "        return self.act()(out)\n",
    "\n",
    "class InspoResNetVAEEncoder(nn.Module):\n",
    "    def __init__(self, in_channels, latent_dim=3, base_channels=32, blocks_per_level=2, use_skips=True, use_bn=True,\n",
    "                 act=nn.GELU, groups=1, dropout=0.4):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, base_channels, 3, padding=1, bias=not use_bn)\n",
    "        self.bn1 = nn.BatchNorm2d(base_channels) if use_bn else nn.Identity()\n",
    "        channels = [base_channels, base_channels * 2, base_channels * 4]\n",
    "        self.levels = nn.ModuleList(\n",
    "            [nn.ModuleList([ResidualBlock(ch, use_skips, use_bn, act=act, dropout=dropout, groups=groups) for _ in range(blocks_per_level)]) for ch in\n",
    "             channels])\n",
    "        self.transitions = nn.ModuleList(\n",
    "            [nn.Conv2d(channels[i], channels[i + 1], 1, bias=not use_bn) for i in range(len(channels) - 1)])\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(base_channels * 4, 2 * latent_dim)\n",
    "        self.act = act\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act()(self.bn1(self.conv1(x)))\n",
    "        for i in range(len(self.levels)):\n",
    "            if i > 0:\n",
    "                x = F.avg_pool2d(x, 2)\n",
    "                x = self.transitions[i - 1](x)\n",
    "            for block in self.levels[i]:\n",
    "                x = block(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = self.fc(x.flatten(start_dim=1))\n",
    "        mean, logvar = x.chunk(2, dim=1)\n",
    "        return mean, logvar\n",
    "\n",
    "class InspoResNetVAEDecoder(nn.Module):\n",
    "    def __init__(self, out_channels, latent_dim=3, base_channels=32, blocks_per_level=2, use_skips=True, use_bn=True,\n",
    "                 act=nn.GELU, groups=1, dropout=0.4):\n",
    "        super().__init__()\n",
    "        channels = [base_channels, base_channels * 2, base_channels * 4][::-1]\n",
    "        self.channels = channels\n",
    "        self.start_dim = 7\n",
    "        self.fc = nn.Linear(latent_dim, channels[0] * self.start_dim * self.start_dim)\n",
    "        self.levels = nn.ModuleList(\n",
    "            [nn.ModuleList([ResidualBlock(ch, use_skips, use_bn, act=act, dropout=dropout, groups=groups) for _ in range(blocks_per_level)]) for ch in\n",
    "             channels])\n",
    "        self.transitions = nn.ModuleList(\n",
    "            [nn.Conv2d(channels[i], channels[i + 1], 1, bias=not use_bn) for i in range(len(channels) - 1)])\n",
    "        self.final_conv = nn.Conv2d(base_channels, out_channels, 3, padding=1)\n",
    "        self.act = act\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.fc(z).view(-1, self.channels[0], self.start_dim, self.start_dim)\n",
    "        for i in range(len(self.levels)):\n",
    "            for block in self.levels[i]:\n",
    "                x = block(x)\n",
    "            if i < len(self.levels) - 1:\n",
    "                x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "                x = self.transitions[i](x)\n",
    "        return self.final_conv(x)\n",
    "\n",
    "class InspoResNetVAE(nn.Module):\n",
    "    def __init__(self, latent_shape=(3,), act=nn.GELU, use_skips=True, use_bn=True, base_channels=32, blocks_per_level=3, groups=1, dropout=0.4):\n",
    "        super().__init__()\n",
    "        if isinstance(latent_shape, int):\n",
    "            latent_shape = (latent_shape,)\n",
    "\n",
    "        self.channels = 1\n",
    "        self.latent_dim = latent_shape[0] if len(latent_shape) == 1 else latent_shape[0] * latent_shape[1] * latent_shape[2]\n",
    "        self.latent_shape = latent_shape\n",
    "        self.act = act\n",
    "        self.use_skips = use_skips\n",
    "        self.use_bn = use_bn\n",
    "        self.base_channels = base_channels\n",
    "        self.blocks_per_level = blocks_per_level\n",
    "        self.groups = groups\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.encoder = InspoResNetVAEEncoder(\n",
    "            in_channels=self.channels, latent_dim=self.latent_dim, base_channels=self.base_channels,\n",
    "            blocks_per_level=self.blocks_per_level,\n",
    "            use_skips=self.use_skips, use_bn=self.use_bn, act=self.act, groups=self.groups, dropout=self.dropout)\n",
    "        self.decoder = InspoResNetVAEDecoder(\n",
    "            out_channels=self.channels, latent_dim=self.latent_dim, base_channels=self.base_channels,\n",
    "            blocks_per_level=self.blocks_per_level,\n",
    "            use_skips=self.use_skips, use_bn=self.use_bn, act=self.act, groups=self.groups, dropout=self.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x)\n",
    "        z = torch.cat([mu, log_var], dim=1)\n",
    "        z_hat = mu + torch.exp(0.5 * log_var) * torch.randn_like(mu)\n",
    "        x_hat = self.decoder(z_hat)\n",
    "        return z, x_hat, mu, log_var, z_hat\n",
    "\n",
    "class FlatVelocityNet(nn.Module):\n",
    "    def __init__(self, input_dim, h_dim=64):\n",
    "        super().__init__()\n",
    "        self.fc_in = nn.Linear(input_dim + 1, h_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, h_dim)\n",
    "        self.fc3 = nn.Linear(h_dim, h_dim)\n",
    "        self.fc_out = nn.Linear(h_dim, input_dim)\n",
    "\n",
    "    def forward(self, x, t, act=F.gelu):\n",
    "        t = t.expand(x.size(0), 1)\n",
    "        x = torch.cat([x, t], dim=1)\n",
    "        x = act(self.fc_in(x))\n",
    "        x = act(self.fc2(x))\n",
    "        x = act(self.fc3(x))\n",
    "        return self.fc_out(x)"
   ],
   "id": "804c89cbef5a9fbb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ],
   "id": "8327b57d79ed585"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def rk4_step(f, y, t, dt):\n",
    "    k1 = f(y, t)\n",
    "    k2 = f(y + 0.5 * dt * k1, t + 0.5 * dt)\n",
    "    k3 = f(y + 0.5 * dt * k2, t + 0.5 * dt)\n",
    "    k4 = f(y + dt * k3, t + dt)\n",
    "    return y + (dt / 6) * (k1 + 2 * k2 + 2 * k3 + k4)\n",
    "\n",
    "@torch.no_grad()\n",
    "def integrate_path(model, initial_points, step_fn=rk4_step, n_steps=100, warp_fn=None, latent_2d=False):\n",
    "    p = next(model.parameters())\n",
    "    device, model_dtype = p.device, p.dtype\n",
    "\n",
    "    current_points = initial_points.to(device=device, dtype=model_dtype).clone()\n",
    "    model.eval()\n",
    "\n",
    "    ts = torch.linspace(0, 1, n_steps, device=device, dtype=model_dtype)\n",
    "    if warp_fn: ts = warp_fn(ts)\n",
    "    if latent_2d: t_batch = torch.empty((current_points.shape[0], 1), device=device, dtype=model_dtype)\n",
    "\n",
    "    for i in range(len(ts) - 1):\n",
    "        t, dt = ts[i], ts[i + 1] - ts[i]\n",
    "        if latent_2d: t = t_batch.fill_(t.item())\n",
    "        current_points = step_fn(model, current_points, t, dt)\n",
    "\n",
    "    return current_points"
   ],
   "id": "7eb27a2a1779fac9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ],
   "id": "96794092a52451e4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE Configuration (matching wise-feather-17)\n",
    "vae_config = {\n",
    "    \"latent_shape\": (16,),\n",
    "    \"base_channels\": 16,\n",
    "    \"blocks_per_level\": 2,\n",
    "    \"groups\": 4,\n",
    "    \"dropout\": 0.3,\n",
    "    \"act\": nn.GELU,\n",
    "    \"use_skips\": True,\n",
    "    \"use_bn\": True\n",
    "}\n",
    "\n",
    "# Training hyperparameters\n",
    "vae_train_config = {\n",
    "    \"batch_size\": 128,\n",
    "    \"learning_rate\": 2e-3,\n",
    "    \"weight_decay\": 5e-5,\n",
    "    \"epochs\": 25,\n",
    "    \"beta_final\": 1.0,\n",
    "    \"warmup_epochs\": 5,\n",
    "}\n",
    "\n",
    "flow_train_config = {\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"epochs\": 50,\n",
    "    \"batch_size\": 128,\n",
    "    \"n_steps\": 100,\n",
    "}"
   ],
   "id": "f803772cbf554be5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load MNIST Data"
   ],
   "id": "3119af09b2011b92"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = MNIST(root='./data', train=True, download=True, transform=ToTensor())\n",
    "test_ds = MNIST(root='./data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=vae_train_config['batch_size'], shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(test_ds, batch_size=vae_train_config['batch_size'], shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Training samples: {len(train_ds)}\")\n",
    "print(f\"Validation samples: {len(test_ds)}\")"
   ],
   "id": "df02a2c4091f1ed6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train VAE"
   ],
   "id": "e8984ad567dfa1d9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb\n",
    "wandb.finish()\n",
    "wandb.init(project=\"reproduce_final_submission\", config=vae_train_config)\n",
    "wandb.config.update(vae_config)\n",
    "\n",
    "# Create VAE model\n",
    "vae = InspoResNetVAE(**vae_config).to(device)\n",
    "total_params = sum(p.numel() for p in vae.parameters())\n",
    "print(f\"VAE parameters: {total_params:,}\")\n",
    "\n",
    "# Update wandb config with model info\n",
    "wandb.config.update({\n",
    "    \"total_params\": total_params,\n",
    "    \"latent_dim\": vae.latent_dim,\n",
    "    \"model_class\": vae.__class__.__name__\n",
    "})\n",
    "\n",
    "# Training setup\n",
    "optimizer = optim.Adam(vae.parameters(), lr=vae_train_config['learning_rate'], weight_decay=vae_train_config['weight_decay'])\n",
    "ema = ExponentialMovingAverage(vae.parameters(), decay=0.9999)\n",
    "recon_criterion = F.binary_cross_entropy_with_logits\n",
    "\n",
    "spatial = len(vae.latent_shape) > 1 if hasattr(vae, 'latent_shape') else False\n",
    "global_step = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(vae_train_config['epochs']):\n",
    "    vae.train()\n",
    "    beta = vae_train_config['beta_final'] * min((epoch + 1) / vae_train_config['warmup_epochs'], 1.0)\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{vae_train_config['epochs']}\", leave=False)\n",
    "    for batch_idx, (data, _) in enumerate(pbar):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        z, x_hat, mu, log_var, z_hat = vae(data)\n",
    "\n",
    "        recon_loss = recon_criterion(x_hat, data, reduction=\"sum\") / data.size(0)\n",
    "        kl = -0.5 * (1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        kl = kl.view(kl.size(0), -1).sum(dim=1).mean()\n",
    "\n",
    "        loss = recon_loss + beta * kl\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ema.update()\n",
    "\n",
    "        wandb.log({\n",
    "            \"step\": global_step,\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"val_mode\": False,\n",
    "            \"train_loss\": loss.item(),\n",
    "            \"recon_loss\": recon_loss.item(),\n",
    "            \"kl_loss\": kl.item(),\n",
    "            \"beta\": beta,\n",
    "            \"beta*kl\": (beta * kl).item(),\n",
    "        })\n",
    "        global_step += 1\n",
    "        pbar.set_postfix(Loss=f\"{loss.item():.4f}\", Recon=f\"{recon_loss.item():.4f}\", KLw=f\"{(beta*kl).item():.5f}\")\n",
    "\n",
    "    # Validation with EMA\n",
    "    vae.eval()\n",
    "    with ema.average_parameters():\n",
    "        val_loss = val_recon = val_kl = 0.0\n",
    "        mu_stats = []\n",
    "        mse_total = mae_total = ssim_total = psnr_total = 0.0\n",
    "        num_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, _ in val_loader:\n",
    "                data = data.to(device)\n",
    "                z, x_hat, mu, log_var, z_hat = vae(data)\n",
    "\n",
    "                recon = recon_criterion(x_hat, data)\n",
    "                kl = -0.5 * torch.mean(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "                loss = recon + beta * kl\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                val_recon += recon.item()\n",
    "                val_kl += kl.item()\n",
    "                mu_stats.append(mu)\n",
    "\n",
    "                # Reconstruction quality metrics (on sigmoid output)\n",
    "                x_hat_sigmoid = torch.sigmoid(x_hat)\n",
    "\n",
    "                # MSE (Mean Squared Error)\n",
    "                mse = F.mse_loss(x_hat_sigmoid, data, reduction='sum')\n",
    "                mse_total += mse.item()\n",
    "\n",
    "                # MAE (Mean Absolute Error)\n",
    "                mae = F.l1_loss(x_hat_sigmoid, data, reduction='sum')\n",
    "                mae_total += mae.item()\n",
    "\n",
    "                # PSNR (Peak Signal-to-Noise Ratio) - higher is better\n",
    "                psnr = 10 * torch.log10(1.0 / (F.mse_loss(x_hat_sigmoid, data) + 1e-8))\n",
    "                psnr_total += psnr.item() * data.size(0)\n",
    "\n",
    "                # Simple SSIM approximation (variance-based similarity)\n",
    "                data_flat = data.view(data.size(0), -1)\n",
    "                recon_flat = x_hat_sigmoid.view(data.size(0), -1)\n",
    "\n",
    "                # Pearson correlation coefficient\n",
    "                data_mean = data_flat.mean(dim=1, keepdim=True)\n",
    "                recon_mean = recon_flat.mean(dim=1, keepdim=True)\n",
    "                data_centered = data_flat - data_mean\n",
    "                recon_centered = recon_flat - recon_mean\n",
    "                correlation = (data_centered * recon_centered).sum(dim=1) / (\n",
    "                    torch.sqrt((data_centered**2).sum(dim=1)) * torch.sqrt((recon_centered**2).sum(dim=1)) + 1e-8\n",
    "                )\n",
    "                ssim_total += correlation.sum().item()\n",
    "\n",
    "                num_samples += data.size(0)\n",
    "\n",
    "        n = len(val_loader)\n",
    "        val_loss /= n\n",
    "        val_recon /= n\n",
    "        val_kl /= n\n",
    "\n",
    "        # Average reconstruction metrics\n",
    "        mse_avg = mse_total / num_samples\n",
    "        mae_avg = mae_total / num_samples\n",
    "        psnr_avg = psnr_total / num_samples\n",
    "        ssim_avg = ssim_total / num_samples\n",
    "\n",
    "        mu_all = torch.cat(mu_stats, dim=0)\n",
    "        mu_mean = mu_all.mean().item()\n",
    "        mu_std = mu_all.std().item()\n",
    "\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"val_mode\": True,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_recon_loss\": val_recon,\n",
    "            \"val_kl_loss\": val_kl,\n",
    "            \"mu_mean\": mu_mean,\n",
    "            \"mu_std\": mu_std,\n",
    "            \"val_mse\": mse_avg,\n",
    "            \"val_mae\": mae_avg,\n",
    "            \"val_psnr\": psnr_avg,\n",
    "            \"val_correlation\": ssim_avg,\n",
    "        })\n",
    "\n",
    "        print(f\"Epoch {epoch+1} - Val Loss: {val_loss:.4f}, Recon: {val_recon:.4f}, KL: {val_kl:.4f}\")\n",
    "        print(f\"  MSE: {mse_avg:.4f}, MAE: {mae_avg:.4f}, PSNR: {psnr_avg:.2f}, Corr: {ssim_avg:.4f}\")\n",
    "        print(f\"  mu_mean: {mu_mean:.4f}, mu_std: {mu_std:.4f}\")\n",
    "\n",
    "print(\"VAE training complete!\")\n",
    "wandb.finish()"
   ],
   "id": "c491d7d4e5187ab6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Google Drive Save and Share Helper",
   "id": "403fad0ff9cfcfa7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def save_and_share(state_dict, filename='model.safetensors', overwrite=True):\n    \"\"\"Save model state dict to Google Drive and return shareable link\"\"\"\n    from safetensors.torch import save_file\n    try:\n        from google.colab import drive, auth\n        from googleapiclient.discovery import build\n        is_colab = True\n    except ImportError:\n        is_colab = False\n    \n    if is_colab:\n        # Colab: Save to Google Drive\n        if not os.path.exists('/content/drive'):\n            drive.mount('/content/drive')\n        \n        output_dir = '/content/drive/MyDrive/vae_models'\n        os.makedirs(output_dir, exist_ok=True)\n        output_path = os.path.join(output_dir, filename)\n        \n        if overwrite and os.path.exists(output_path):\n            os.remove(output_path)\n            print(f\"Deleted old version of {filename}\")\n        \n        save_file(state_dict, output_path)\n        print(f\"Model saved to: {output_path}\")\n        \n        # Get shareable link\n        auth.authenticate_user()\n        service = build('drive', 'v3')\n        \n        results = service.files().list(\n            q=f\"name='{filename}'\",\n            fields='files(id)'\n        ).execute()\n        \n        if results.get('files'):\n            file_id = results['files'][0]['id']\n            service.permissions().create(\n                fileId=file_id,\n                body={'type': 'anyone', 'role': 'reader'}\n            ).execute()\n            link = f\"https://drive.google.com/file/d/{file_id}/view?usp=sharing\"\n            print(f\"Shareable link: {link}\")\n            return link\n    else:\n        # Local: Save to models/safetensors\n        output_dir = 'models/safetensors'\n        os.makedirs(output_dir, exist_ok=True)\n        output_path = os.path.join(output_dir, filename)\n        \n        save_file(state_dict, output_path)\n        print(f\"Model saved to: {output_path}\")\n        return output_path\n    \n    return None",
   "id": "c5d48e718fda1d1b"
  },
  {
   "cell_type": "markdown",
   "id": "ucfjvvw8pdc",
   "source": "## Step 3: Save VAE with EMA Weights",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "zs7mlg1zqze",
   "source": "# Extract and save EMA weights\nwith ema.average_parameters():\n    vae_state_dict = {k: v.cpu().clone() for k, v in vae.state_dict().items()}\n\nvae_link = save_and_share(vae_state_dict, filename='vae_flat_16_ema.safetensors')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Encode MNIST Dataset"
   ],
   "id": "3c5fb97796c19acb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "def encode_dataset(model, dataloader, ema=None):\n",
    "    model.eval()\n",
    "    latents = []\n",
    "    labels = []\n",
    "    \n",
    "    context = ema.average_parameters() if ema is not None else torch.no_grad()\n",
    "    \n",
    "    with context:\n",
    "        for data, label in tqdm(dataloader, desc=\"Encoding dataset\"):\n",
    "            data = data.to(device)\n",
    "            mu, _ = model.encoder(data)\n",
    "            latents.append(mu.cpu())\n",
    "            labels.append(label)\n",
    "    \n",
    "    latents = torch.cat(latents, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    return TensorDataset(latents, labels)\n",
    "\n",
    "# Encode both train and test sets\n",
    "encoded_train = encode_dataset(vae, train_loader, ema=ema)\n",
    "encoded_test = encode_dataset(vae, val_loader, ema=ema)\n",
    "\n",
    "print(f\"Encoded train shape: {encoded_train.tensors[0].shape}\")\n",
    "print(f\"Encoded test shape: {encoded_test.tensors[0].shape}\")\n",
    "\n",
    "# Create dataloaders for flow training\n",
    "flow_train_loader = DataLoader(encoded_train, batch_size=flow_train_config['batch_size'], shuffle=True)\n",
    "flow_val_loader = DataLoader(encoded_test, batch_size=flow_train_config['batch_size'], shuffle=False)"
   ],
   "id": "c727c5c53c90a4a6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 5: Train Flow Model",
   "id": "d913d46509a9510d"
  },
  {
   "cell_type": "code",
   "id": "9mk2ehgyrz",
   "source": "# Initialize wandb for flow training\nwandb.init(project=\"reproduce_final_submission\", config=flow_train_config)\n\n# Create flow model\nflow_model = FlatVelocityNet(input_dim=16, h_dim=64).to(device)\ntotal_params = sum(p.numel() for p in flow_model.parameters())\nprint(f\"Flow model parameters: {total_params:,}\")\n\nwandb.config.update({\n    \"total_params\": total_params,\n    \"model_class\": flow_model.__class__.__name__,\n    \"input_dim\": 16,\n    \"h_dim\": 64\n})\n\n# Training setup\ncriterion = nn.MSELoss()\noptimizer_flow = optim.Adam(flow_model.parameters(), lr=flow_train_config['learning_rate'])\nema_flow = ExponentialMovingAverage(flow_model.parameters(), decay=0.9999)\n\nglobal_step = 0\nspatial = False  # Flat latents, not spatial\n\n# Training loop\nfor epoch in range(flow_train_config['epochs']):\n    flow_model.train()\n    \n    pbar = tqdm(flow_train_loader, desc=f\"Flow Epoch {epoch+1}/{flow_train_config['epochs']}\", leave=False)\n    for batch_idx, (data, _) in enumerate(pbar):\n        optimizer_flow.zero_grad()\n        \n        # Get data and create noise sample\n        target_x = data.to(device)\n        sampled_x = torch.randn_like(target_x)\n        B = sampled_x.size(0)\n        \n        # Sample random timestep\n        t = torch.rand(B, 1, device=device, dtype=target_x.dtype)\n        \n        # Interpolate between noise and data\n        if spatial:\n            t4 = t.view(B, 1, 1, 1)\n            interpolated_x = sampled_x * (1 - t4) + target_x * t4\n        else:\n            interpolated_x = sampled_x * (1 - t) + target_x * t\n        \n        # Compute target direction\n        line_directions = target_x - sampled_x\n        \n        # Predict drift\n        drift = flow_model(interpolated_x, t)\n        loss = criterion(drift, line_directions)\n        \n        loss.backward()\n        optimizer_flow.step()\n        ema_flow.update()\n        \n        # Metrics\n        with torch.no_grad():\n            cos_sim = F.cosine_similarity(drift, line_directions, dim=1).mean()\n            drift_norm = drift.norm(dim=1).mean()\n        \n        wandb.log({\n            \"step\": global_step,\n            \"epoch\": epoch + 1,\n            \"train_loss\": loss.item(),\n            \"cos_sim\": cos_sim.item(),\n            \"drift_norm\": drift_norm.item(),\n        })\n        global_step += 1\n        pbar.set_postfix(Loss=f\"{loss.item():.4f}\", CosSim=f\"{cos_sim.item():.3f}\", Drift=f\"{drift_norm.item():.3f}\")\n    \n    # Validation every 5 epochs\n    if (epoch + 1) % 5 == 0:\n        flow_model.eval()\n        with ema_flow.average_parameters():\n            val_loss = 0.0\n            val_cos_sim = 0.0\n            n_batches = 0\n            \n            with torch.no_grad():\n                for data, _ in flow_val_loader:\n                    target_x = data.to(device)\n                    sampled_x = torch.randn_like(target_x)\n                    B = sampled_x.size(0)\n                    \n                    t = torch.rand(B, 1, device=device, dtype=target_x.dtype)\n                    \n                    if spatial:\n                        t4 = t.view(B, 1, 1, 1)\n                        interpolated_x = sampled_x * (1 - t4) + target_x * t4\n                    else:\n                        interpolated_x = sampled_x * (1 - t) + target_x * t\n                    \n                    line_directions = target_x - sampled_x\n                    drift = flow_model(interpolated_x, t)\n                    loss = criterion(drift, line_directions)\n                    cos_sim = F.cosine_similarity(drift, line_directions, dim=1).mean()\n                    \n                    val_loss += loss.item()\n                    val_cos_sim += cos_sim.item()\n                    n_batches += 1\n            \n            wandb.log({\n                \"epoch\": epoch + 1,\n                \"val_loss\": val_loss / n_batches,\n                \"val_cos_sim\": val_cos_sim / n_batches,\n            })\n            print(f\"Epoch {epoch+1} - Val Loss: {val_loss/n_batches:.4f}, CosSim: {val_cos_sim/n_batches:.3f}\")\n\nprint(\"Flow training complete!\")\nwandb.finish()",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract and save EMA weights\nwith ema_flow.average_parameters():\n    flow_state_dict = {k: v.cpu().clone() for k, v in flow_model.state_dict().items()}\n\nflow_link = save_and_share(flow_state_dict, filename='flow_flat_16_ema.safetensors')\nprint(\"Flow model saved!\")",
   "id": "e35f87c9109bc467"
  },
  {
   "cell_type": "markdown",
   "id": "fjaku2l3mvv",
   "source": "## Step 7: Generate Samples",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples\n",
    "n_samples = 64\n",
    "n_steps = 20\n",
    "\n",
    "# Move models back to device if needed\n",
    "vae = vae.to(device)\n",
    "flow_model = flow_model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    with ema_flow.average_parameters():\n",
    "        # Start from noise\n",
    "        z_noise = torch.randn(n_samples, 16, device=device)\n",
    "        \n",
    "        # Integrate through flow to get latents\n",
    "        z_samples = integrate_path(flow_model, z_noise, step_fn=rk4_step, n_steps=n_steps, latent_2d=False)\n",
    "        \n",
    "    with ema.average_parameters():\n",
    "        # Decode latents to images\n",
    "        x_samples = vae.decoder(z_samples)\n",
    "        x_samples = torch.sigmoid(x_samples)\n",
    "        x_samples = x_samples.cpu()\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(8, 8, figsize=(10, 10))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(x_samples[i, 0], cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig('generated_samples.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Generated {n_samples} samples\")\n",
    "print(\"Total parameters (VAE + Flow):\", sum(p.numel() for p in vae.parameters()) + sum(p.numel() for p in flow_model.parameters()))"
   ],
   "id": "1cd10745681e32fa"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Submission Interface Class\n\nThe following class is used for automated evaluation and can load models from Google Drive links.",
   "id": "a9bbcda8e79484f5"
  },
  {
   "cell_type": "code",
   "id": "1s9cno8wqcc",
   "source": "!pip install gdown\n\nimport gdown\n\nclass SubmissionInterface(nn.Module):\n    \"\"\"All teams must implement this for automated evaluation.\n    When you subclass/implement these methods, replace the NotImplementedError.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n        # --- REQUIRED INFO:\n        self.info = {\n            'team': 'marco',\n            'names': 'Marco',\n        }\n        self.latent_dim = 16\n        self.specifications = {\n            \"latent_shape\": (16,),\n            \"base_channels\": 16,\n            \"blocks_per_level\": 2,\n            \"groups\": 4,\n            \"dropout\": 0.3,\n            \"act\": nn.GELU,\n            \"use_skips\": True,\n            \"use_bn\": True\n        }\n        # ----\n\n        # keep support for full auto-initialization:\n        self.device = 'cpu'\n        self.load_vae()\n        self.load_flow_model()\n\n    def load_vae(self):\n        \"\"\"this completely specifies the vae model including configuration parameters,\n            downloads/mounts the weights from Google Drive, automatically loads weights\"\"\"\n        self.vae = InspoResNetVAE(**self.specifications)\n        vae_weights_file = 'downloaded_vae.safetensors'\n        safetensors_link = \"https://drive.google.com/file/d/1rP6yP5yixCI1M7LOrv9v9vJkeYkXnptG/view?usp=drive_link\"\n        gdown.download(safetensors_link, vae_weights_file, quiet=False, fuzzy=True)\n        self.vae.load_state_dict(load_file(vae_weights_file))\n\n    def load_flow_model(self):\n        \"\"\"this completely specifies the flow model including configuration parameters,\n           downloads/mounts the weights from Google Drive, automatically loads weights\"\"\"\n        self.flow_model = FlatVelocityNet(input_dim=self.latent_dim)\n        flow_weights_file = 'downloaded_flow.safetensors'\n        safetensors_link = \"https://drive.google.com/file/d/1pAU4p6xQMDgAuywu-go-5iM4qPDXgJFc/view?usp=drive_link\"\n        gdown.download(safetensors_link, flow_weights_file, quiet=False, fuzzy=True)\n        self.flow_model.load_state_dict(load_file(flow_weights_file))\n\n    def generate_samples(self, n_samples: int, n_steps=15) -> torch.Tensor:\n        z0 = torch.randn([n_samples, self.latent_dim]).to(self.device)\n        z1 = integrate_path(self.flow_model, z0, n_steps=n_steps, step_fn=rk4_step)\n        gen_xhat = F.sigmoid(self.decode(z1).view(-1, 28, 28))\n        return gen_xhat\n\n    def encode(self, images: torch.Tensor) -> torch.Tensor:\n        # if your vae has linear layers, flatten first\n        # if your vae has conv layers, comment out next line\n        # images = images.view(images.size(0), -1)\n        with torch.no_grad():\n            z = self.vae.encoder(images.to(self.device))\n            # mu = z[:, :self.latent_dim]  # return only first half (mu)\n            if isinstance(z, (tuple, list)):\n                mu, _ = z\n            else:\n                mu = z\n            return mu\n\n    def decode(self, latents: torch.Tensor) -> torch.Tensor:\n        return self.vae.decoder(latents)\n\n    def to(self, device):\n        self.device = device\n        self.vae.to(self.device)\n        self.flow_model.to(self.device)\n        return self\n\n# Sample usage:\n# device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n# mysub = SubmissionInterface().to(device) # loads vae and flow models\n# xhat_gen = mysub.generate_samples(n_samples=10, n_steps=100)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}