{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c4f7de9b93542b2"
      },
      "source": [
        "# Reproduce Final Submission - Full Training Pipeline\n",
        "\n",
        "**Note:** This notebook was generated by Claude Code on behalf of Marco Cassar. The original training repository was provided to compile this notebook version for ease of access in Google Colab.\n",
        "\n",
        "This notebook trains a VAE + Flow model from scratch following the exact pipeline:\n",
        "1. Train VAE on MNIST\n",
        "2. Save VAE with EMA weights\n",
        "3. Encode MNIST dataset using trained VAE\n",
        "4. Train flow model on encoded latents\n",
        "5. Save flow model with EMA weights\n",
        "6. Test generation"
      ],
      "id": "7c4f7de9b93542b2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80cc28ee05e55c9e"
      },
      "source": [
        "## Imports and Setup"
      ],
      "id": "80cc28ee05e55c9e"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d19a3c966120b81",
        "outputId": "2f1e034e-9918-45ac-98ca-9c11ece48505"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.22.2)\n",
            "Collecting torch_ema\n",
            "  Downloading torch_ema-0.3-py3-none-any.whl.metadata (415 bytes)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (25.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.11.10)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.40.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from torch_ema) (2.8.0+cu126)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2025.10.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->torch_ema) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->torch_ema) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->torch_ema) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->torch_ema) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->torch_ema) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->torch_ema) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torch_ema) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torch_ema) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->torch_ema) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->torch_ema) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->torch_ema) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->torch_ema) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->torch_ema) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->torch_ema) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->torch_ema) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->torch_ema) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->torch_ema) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torch_ema) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->torch_ema) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->torch_ema) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->torch_ema) (3.4.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->torch_ema) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->torch_ema) (3.0.3)\n",
            "Downloading torch_ema-0.3-py3-none-any.whl (5.5 kB)\n",
            "Installing collected packages: torch_ema\n",
            "Successfully installed torch_ema-0.3\n"
          ]
        }
      ],
      "execution_count": 1,
      "source": [
        "!pip install wandb torch_ema"
      ],
      "id": "d19a3c966120b81"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f724ab790530c38",
        "outputId": "87919779-9fb1-41c6-c3a9-bf4ff539fddb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch_ema import ExponentialMovingAverage\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import ToTensor\n",
        "from tqdm import tqdm\n",
        "from safetensors.torch import save_file, load_file\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "from math import log2\n",
        "\n",
        "# Device setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ],
      "id": "9f724ab790530c38"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9574089a9e7a55d2"
      },
      "source": [
        "## Model Definitions"
      ],
      "id": "9574089a9e7a55d2"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "804c89cbef5a9fbb"
      },
      "outputs": [],
      "source": [
        "# Copy all model classes from marco_submission_2.py\n",
        "# ResidualBlock, InspoResNetVAEEncoder, InspoResNetVAEDecoder, InspoResNetVAE, FlatVelocityNet\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels, use_skip=True, use_bn=True, act=nn.GELU, dropout=0.4, groups=1):\n",
        "        super().__init__()\n",
        "        self.dropout = dropout\n",
        "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1, bias=not use_bn, groups=groups)\n",
        "        self.bn1 = nn.BatchNorm2d(channels) if use_bn else nn.Identity()\n",
        "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1, bias=not use_bn, groups=groups)\n",
        "        self.bn2 = nn.BatchNorm2d(channels) if use_bn else nn.Identity()\n",
        "        self.use_skip, self.act = use_skip, act\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_skip: x0 = x\n",
        "        out = self.act()(self.bn1(self.conv1(x)))\n",
        "        out = F.dropout(out, self.dropout, training=self.training)\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        if self.use_skip: out = out + x0\n",
        "        return self.act()(out)\n",
        "\n",
        "class InspoResNetVAEEncoder(nn.Module):\n",
        "    def __init__(self, in_channels, latent_dim=3, base_channels=32, blocks_per_level=2, use_skips=True, use_bn=True,\n",
        "                 act=nn.GELU, groups=1, dropout=0.4):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, base_channels, 3, padding=1, bias=not use_bn)\n",
        "        self.bn1 = nn.BatchNorm2d(base_channels) if use_bn else nn.Identity()\n",
        "        channels = [base_channels, base_channels * 2, base_channels * 4]\n",
        "        self.levels = nn.ModuleList(\n",
        "            [nn.ModuleList([ResidualBlock(ch, use_skips, use_bn, act=act, dropout=dropout, groups=groups) for _ in range(blocks_per_level)]) for ch in\n",
        "             channels])\n",
        "        self.transitions = nn.ModuleList(\n",
        "            [nn.Conv2d(channels[i], channels[i + 1], 1, bias=not use_bn) for i in range(len(channels) - 1)])\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(base_channels * 4, 2 * latent_dim)\n",
        "        self.act = act\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.act()(self.bn1(self.conv1(x)))\n",
        "        for i in range(len(self.levels)):\n",
        "            if i > 0:\n",
        "                x = F.avg_pool2d(x, 2)\n",
        "                x = self.transitions[i - 1](x)\n",
        "            for block in self.levels[i]:\n",
        "                x = block(x)\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = self.fc(x.flatten(start_dim=1))\n",
        "        mean, logvar = x.chunk(2, dim=1)\n",
        "        return mean, logvar\n",
        "\n",
        "class InspoResNetVAEDecoder(nn.Module):\n",
        "    def __init__(self, out_channels, latent_dim=3, base_channels=32, blocks_per_level=2, use_skips=True, use_bn=True,\n",
        "                 act=nn.GELU, groups=1, dropout=0.4):\n",
        "        super().__init__()\n",
        "        channels = [base_channels, base_channels * 2, base_channels * 4][::-1]\n",
        "        self.channels = channels\n",
        "        self.start_dim = 7\n",
        "        self.fc = nn.Linear(latent_dim, channels[0] * self.start_dim * self.start_dim)\n",
        "        self.levels = nn.ModuleList(\n",
        "            [nn.ModuleList([ResidualBlock(ch, use_skips, use_bn, act=act, dropout=dropout, groups=groups) for _ in range(blocks_per_level)]) for ch in\n",
        "             channels])\n",
        "        self.transitions = nn.ModuleList(\n",
        "            [nn.Conv2d(channels[i], channels[i + 1], 1, bias=not use_bn) for i in range(len(channels) - 1)])\n",
        "        self.final_conv = nn.Conv2d(base_channels, out_channels, 3, padding=1)\n",
        "        self.act = act\n",
        "\n",
        "    def forward(self, z):\n",
        "        x = self.fc(z).view(-1, self.channels[0], self.start_dim, self.start_dim)\n",
        "        for i in range(len(self.levels)):\n",
        "            for block in self.levels[i]:\n",
        "                x = block(x)\n",
        "            if i < len(self.levels) - 1:\n",
        "                x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "                x = self.transitions[i](x)\n",
        "        return self.final_conv(x)\n",
        "\n",
        "class InspoResNetVAE(nn.Module):\n",
        "    def __init__(self, latent_shape=(3,), act=nn.GELU, use_skips=True, use_bn=True, base_channels=32, blocks_per_level=3, groups=1, dropout=0.4):\n",
        "        super().__init__()\n",
        "        if isinstance(latent_shape, int):\n",
        "            latent_shape = (latent_shape,)\n",
        "\n",
        "        self.channels = 1\n",
        "        self.latent_dim = latent_shape[0] if len(latent_shape) == 1 else latent_shape[0] * latent_shape[1] * latent_shape[2]\n",
        "        self.latent_shape = latent_shape\n",
        "        self.act = act\n",
        "        self.use_skips = use_skips\n",
        "        self.use_bn = use_bn\n",
        "        self.base_channels = base_channels\n",
        "        self.blocks_per_level = blocks_per_level\n",
        "        self.groups = groups\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.encoder = InspoResNetVAEEncoder(\n",
        "            in_channels=self.channels, latent_dim=self.latent_dim, base_channels=self.base_channels,\n",
        "            blocks_per_level=self.blocks_per_level,\n",
        "            use_skips=self.use_skips, use_bn=self.use_bn, act=self.act, groups=self.groups, dropout=self.dropout)\n",
        "        self.decoder = InspoResNetVAEDecoder(\n",
        "            out_channels=self.channels, latent_dim=self.latent_dim, base_channels=self.base_channels,\n",
        "            blocks_per_level=self.blocks_per_level,\n",
        "            use_skips=self.use_skips, use_bn=self.use_bn, act=self.act, groups=self.groups, dropout=self.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, log_var = self.encoder(x)\n",
        "        z = torch.cat([mu, log_var], dim=1)\n",
        "        z_hat = mu + torch.exp(0.5 * log_var) * torch.randn_like(mu)\n",
        "        x_hat = self.decoder(z_hat)\n",
        "        return z, x_hat, mu, log_var, z_hat\n",
        "\n",
        "class FlatVelocityNet(nn.Module):\n",
        "    def __init__(self, input_dim, h_dim=64):\n",
        "        super().__init__()\n",
        "        self.fc_in = nn.Linear(input_dim + 1, h_dim)\n",
        "        self.fc2 = nn.Linear(h_dim, h_dim)\n",
        "        self.fc3 = nn.Linear(h_dim, h_dim)\n",
        "        self.fc_out = nn.Linear(h_dim, input_dim)\n",
        "\n",
        "    def forward(self, x, t, act=F.gelu):\n",
        "        t = t.expand(x.size(0), 1)\n",
        "        x = torch.cat([x, t], dim=1)\n",
        "        x = act(self.fc_in(x))\n",
        "        x = act(self.fc2(x))\n",
        "        x = act(self.fc3(x))\n",
        "        return self.fc_out(x)"
      ],
      "id": "804c89cbef5a9fbb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8327b57d79ed585"
      },
      "source": [
        "## Helper Functions"
      ],
      "id": "8327b57d79ed585"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7eb27a2a1779fac9"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def rk4_step(f, y, t, dt):\n",
        "    k1 = f(y, t)\n",
        "    k2 = f(y + 0.5 * dt * k1, t + 0.5 * dt)\n",
        "    k3 = f(y + 0.5 * dt * k2, t + 0.5 * dt)\n",
        "    k4 = f(y + dt * k3, t + dt)\n",
        "    return y + (dt / 6) * (k1 + 2 * k2 + 2 * k3 + k4)\n",
        "\n",
        "@torch.no_grad()\n",
        "def integrate_path(model, initial_points, step_fn=rk4_step, n_steps=100, warp_fn=None, latent_2d=False):\n",
        "    p = next(model.parameters())\n",
        "    device, model_dtype = p.device, p.dtype\n",
        "\n",
        "    current_points = initial_points.to(device=device, dtype=model_dtype).clone()\n",
        "    model.eval()\n",
        "\n",
        "    ts = torch.linspace(0, 1, n_steps, device=device, dtype=model_dtype)\n",
        "    if warp_fn: ts = warp_fn(ts)\n",
        "    if latent_2d: t_batch = torch.empty((current_points.shape[0], 1), device=device, dtype=model_dtype)\n",
        "\n",
        "    for i in range(len(ts) - 1):\n",
        "        t, dt = ts[i], ts[i + 1] - ts[i]\n",
        "        if latent_2d: t = t_batch.fill_(t.item())\n",
        "        current_points = step_fn(model, current_points, t, dt)\n",
        "\n",
        "    return current_points"
      ],
      "id": "7eb27a2a1779fac9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96794092a52451e4"
      },
      "source": [
        "## Configuration"
      ],
      "id": "96794092a52451e4"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "f803772cbf554be5"
      },
      "outputs": [],
      "source": [
        "# VAE Configuration (matching wise-feather-17)\n",
        "vae_config = {\n",
        "    \"latent_shape\": (16,),\n",
        "    \"base_channels\": 16,\n",
        "    \"blocks_per_level\": 2,\n",
        "    \"groups\": 4,\n",
        "    \"dropout\": 0.3,\n",
        "    \"act\": nn.GELU,\n",
        "    \"use_skips\": True,\n",
        "    \"use_bn\": True\n",
        "}\n",
        "\n",
        "# Training hyperparameters\n",
        "vae_train_config = {\n",
        "    \"batch_size\": 128,\n",
        "    \"learning_rate\": 2e-3,\n",
        "    \"weight_decay\": 5e-5,\n",
        "    # \"epochs\": 25,\n",
        "    \"epochs\": 1,\n",
        "    \"beta_final\": 1.0,\n",
        "    \"warmup_epochs\": 5,\n",
        "}\n",
        "\n",
        "flow_train_config = {\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"weight_decay\": 0.0,\n",
        "    # \"epochs\": 50,\n",
        "    \"epochs\": 1,\n",
        "    \"batch_size\": 128,\n",
        "    \"n_steps\": 100,\n",
        "}"
      ],
      "id": "f803772cbf554be5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3119af09b2011b92"
      },
      "source": [
        "## Step 1: Load MNIST Data"
      ],
      "id": "3119af09b2011b92"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df02a2c4091f1ed6",
        "outputId": "01146abf-d978-47b8-e7d3-3caaecf60c6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:02<00:00, 4.52MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 131kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.24MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 12.3MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 60000\n",
            "Validation samples: 10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "train_ds = MNIST(root='./data', train=True, download=True, transform=ToTensor())\n",
        "test_ds = MNIST(root='./data', train=False, download=True, transform=ToTensor())\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=vae_train_config['batch_size'], shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(test_ds, batch_size=vae_train_config['batch_size'], shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Training samples: {len(train_ds)}\")\n",
        "print(f\"Validation samples: {len(test_ds)}\")"
      ],
      "id": "df02a2c4091f1ed6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8984ad567dfa1d9"
      },
      "source": [
        "## Step 2: Train VAE"
      ],
      "id": "e8984ad567dfa1d9"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c491d7d4e5187ab6",
        "outputId": "e284851d-f93c-4703-b70a-f32843f6ab22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarcocassar\u001b[0m (\u001b[33mmarcocassar-belmont-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251012_233807-1ay77mrh</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/marcocassar-belmont-university/reproduce_final_submission/runs/1ay77mrh' target=\"_blank\">gentle-waterfall-4</a></strong> to <a href='https://wandb.ai/marcocassar-belmont-university/reproduce_final_submission' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/marcocassar-belmont-university/reproduce_final_submission' target=\"_blank\">https://wandb.ai/marcocassar-belmont-university/reproduce_final_submission</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/marcocassar-belmont-university/reproduce_final_submission/runs/1ay77mrh' target=\"_blank\">https://wandb.ai/marcocassar-belmont-university/reproduce_final_submission/runs/1ay77mrh</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAE parameters: 159,393\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Val Loss: 0.5221, Recon: 0.1268, KL: 1.9761\n",
            "  MSE: 16.7544, MAE: 48.0105, PSNR: 16.71, Corr: 0.8762\n",
            "  mu_mean: -0.0106, mu_std: 0.9218\n",
            "VAE training complete!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>beta</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>beta*kl</td><td>▁▁▁▂▃▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇██████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>kl_loss</td><td>▁▃▃▄▄▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇███▇███████████████</td></tr><tr><td>mu_mean</td><td>▁</td></tr><tr><td>mu_std</td><td>▁</td></tr><tr><td>recon_loss</td><td>█▇▆▆▅▅▄▄▄▃▃▃▂▂▃▃▂▂▂▂▂▂▂▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▂▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_correlation</td><td>▁</td></tr><tr><td>+6</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>beta</td><td>0.2</td></tr><tr><td>beta*kl</td><td>6.77269</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>kl_loss</td><td>33.86344</td></tr><tr><td>mu_mean</td><td>-0.01064</td></tr><tr><td>mu_std</td><td>0.92183</td></tr><tr><td>recon_loss</td><td>107.31058</td></tr><tr><td>step</td><td>468</td></tr><tr><td>train_loss</td><td>114.08327</td></tr><tr><td>val_correlation</td><td>0.87621</td></tr><tr><td>+7</td><td>...</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">gentle-waterfall-4</strong> at: <a href='https://wandb.ai/marcocassar-belmont-university/reproduce_final_submission/runs/1ay77mrh' target=\"_blank\">https://wandb.ai/marcocassar-belmont-university/reproduce_final_submission/runs/1ay77mrh</a><br> View project at: <a href='https://wandb.ai/marcocassar-belmont-university/reproduce_final_submission' target=\"_blank\">https://wandb.ai/marcocassar-belmont-university/reproduce_final_submission</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251012_233807-1ay77mrh/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Initialize wandb\n",
        "wandb.finish()\n",
        "wandb.init(project=\"reproduce_final_submission\", config=vae_train_config)\n",
        "wandb.config.update(vae_config)\n",
        "\n",
        "# Create VAE model\n",
        "vae = InspoResNetVAE(**vae_config).to(device)\n",
        "total_params = sum(p.numel() for p in vae.parameters())\n",
        "print(f\"VAE parameters: {total_params:,}\")\n",
        "\n",
        "# Update wandb config with model info\n",
        "wandb.config.update({\n",
        "    \"total_params\": total_params,\n",
        "    \"latent_dim\": vae.latent_dim,\n",
        "    \"model_class\": vae.__class__.__name__\n",
        "})\n",
        "\n",
        "# Training setup\n",
        "optimizer = optim.Adam(vae.parameters(), lr=vae_train_config['learning_rate'], weight_decay=vae_train_config['weight_decay'])\n",
        "ema = ExponentialMovingAverage(vae.parameters(), decay=0.9999)\n",
        "recon_criterion = F.binary_cross_entropy_with_logits\n",
        "\n",
        "spatial = len(vae.latent_shape) > 1 if hasattr(vae, 'latent_shape') else False\n",
        "global_step = 0\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(vae_train_config['epochs']):\n",
        "    vae.train()\n",
        "    beta = vae_train_config['beta_final'] * min((epoch + 1) / vae_train_config['warmup_epochs'], 1.0)\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{vae_train_config['epochs']}\", leave=False)\n",
        "    for batch_idx, (data, _) in enumerate(pbar):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        z, x_hat, mu, log_var, z_hat = vae(data)\n",
        "\n",
        "        recon_loss = recon_criterion(x_hat, data, reduction=\"sum\") / data.size(0)\n",
        "        kl = -0.5 * (1 + log_var - mu.pow(2) - log_var.exp())\n",
        "        kl = kl.view(kl.size(0), -1).sum(dim=1).mean()\n",
        "\n",
        "        loss = recon_loss + beta * kl\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        ema.update()\n",
        "\n",
        "        wandb.log({\n",
        "            \"step\": global_step,\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"val_mode\": False,\n",
        "            \"train_loss\": loss.item(),\n",
        "            \"recon_loss\": recon_loss.item(),\n",
        "            \"kl_loss\": kl.item(),\n",
        "            \"beta\": beta,\n",
        "            \"beta*kl\": (beta * kl).item(),\n",
        "        })\n",
        "        global_step += 1\n",
        "        pbar.set_postfix(Loss=f\"{loss.item():.4f}\", Recon=f\"{recon_loss.item():.4f}\", KLw=f\"{(beta*kl).item():.5f}\")\n",
        "\n",
        "    # Validation with EMA\n",
        "    vae.eval()\n",
        "    with ema.average_parameters():\n",
        "        val_loss = val_recon = val_kl = 0.0\n",
        "        mu_stats = []\n",
        "        mse_total = mae_total = ssim_total = psnr_total = 0.0\n",
        "        num_samples = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, _ in val_loader:\n",
        "                data = data.to(device)\n",
        "                z, x_hat, mu, log_var, z_hat = vae(data)\n",
        "\n",
        "                recon = recon_criterion(x_hat, data)\n",
        "                kl = -0.5 * torch.mean(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "                loss = recon + beta * kl\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                val_recon += recon.item()\n",
        "                val_kl += kl.item()\n",
        "                mu_stats.append(mu)\n",
        "\n",
        "                # Reconstruction quality metrics (on sigmoid output)\n",
        "                x_hat_sigmoid = torch.sigmoid(x_hat)\n",
        "\n",
        "                # MSE (Mean Squared Error)\n",
        "                mse = F.mse_loss(x_hat_sigmoid, data, reduction='sum')\n",
        "                mse_total += mse.item()\n",
        "\n",
        "                # MAE (Mean Absolute Error)\n",
        "                mae = F.l1_loss(x_hat_sigmoid, data, reduction='sum')\n",
        "                mae_total += mae.item()\n",
        "\n",
        "                # PSNR (Peak Signal-to-Noise Ratio) - higher is better\n",
        "                psnr = 10 * torch.log10(1.0 / (F.mse_loss(x_hat_sigmoid, data) + 1e-8))\n",
        "                psnr_total += psnr.item() * data.size(0)\n",
        "\n",
        "                # Simple SSIM approximation (variance-based similarity)\n",
        "                data_flat = data.view(data.size(0), -1)\n",
        "                recon_flat = x_hat_sigmoid.view(data.size(0), -1)\n",
        "\n",
        "                # Pearson correlation coefficient\n",
        "                data_mean = data_flat.mean(dim=1, keepdim=True)\n",
        "                recon_mean = recon_flat.mean(dim=1, keepdim=True)\n",
        "                data_centered = data_flat - data_mean\n",
        "                recon_centered = recon_flat - recon_mean\n",
        "                correlation = (data_centered * recon_centered).sum(dim=1) / (\n",
        "                    torch.sqrt((data_centered**2).sum(dim=1)) * torch.sqrt((recon_centered**2).sum(dim=1)) + 1e-8\n",
        "                )\n",
        "                ssim_total += correlation.sum().item()\n",
        "\n",
        "                num_samples += data.size(0)\n",
        "\n",
        "        n = len(val_loader)\n",
        "        val_loss /= n\n",
        "        val_recon /= n\n",
        "        val_kl /= n\n",
        "\n",
        "        # Average reconstruction metrics\n",
        "        mse_avg = mse_total / num_samples\n",
        "        mae_avg = mae_total / num_samples\n",
        "        psnr_avg = psnr_total / num_samples\n",
        "        ssim_avg = ssim_total / num_samples\n",
        "\n",
        "        mu_all = torch.cat(mu_stats, dim=0)\n",
        "        mu_mean = mu_all.mean().item()\n",
        "        mu_std = mu_all.std().item()\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"val_mode\": True,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_recon_loss\": val_recon,\n",
        "            \"val_kl_loss\": val_kl,\n",
        "            \"mu_mean\": mu_mean,\n",
        "            \"mu_std\": mu_std,\n",
        "            \"val_mse\": mse_avg,\n",
        "            \"val_mae\": mae_avg,\n",
        "            \"val_psnr\": psnr_avg,\n",
        "            \"val_correlation\": ssim_avg,\n",
        "        })\n",
        "\n",
        "        print(f\"Epoch {epoch+1} - Val Loss: {val_loss:.4f}, Recon: {val_recon:.4f}, KL: {val_kl:.4f}\")\n",
        "        print(f\"  MSE: {mse_avg:.4f}, MAE: {mae_avg:.4f}, PSNR: {psnr_avg:.2f}, Corr: {ssim_avg:.4f}\")\n",
        "        print(f\"  mu_mean: {mu_mean:.4f}, mu_std: {mu_std:.4f}\")\n",
        "\n",
        "print(\"VAE training complete!\")\n",
        "wandb.finish()"
      ],
      "id": "c491d7d4e5187ab6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "403fad0ff9cfcfa7"
      },
      "source": [
        "## Google Drive Save and Share Helper"
      ],
      "id": "403fad0ff9cfcfa7"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "c5d48e718fda1d1b"
      },
      "outputs": [],
      "source": [
        "def save_and_share(state_dict, filename='model.safetensors', overwrite=True):\n",
        "    \"\"\"Save model state dict to Google Drive and return shareable link\"\"\"\n",
        "    from safetensors.torch import save_file\n",
        "    try:\n",
        "        from google.colab import drive, auth\n",
        "        from googleapiclient.discovery import build\n",
        "        is_colab = True\n",
        "    except ImportError:\n",
        "        is_colab = False\n",
        "\n",
        "    if is_colab:\n",
        "        # Colab: Save to Google Drive\n",
        "        if not os.path.exists('/content/drive'):\n",
        "            drive.mount('/content/drive')\n",
        "\n",
        "        output_dir = '/content/drive/MyDrive/vae_models'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        output_path = os.path.join(output_dir, filename)\n",
        "\n",
        "        if overwrite and os.path.exists(output_path):\n",
        "            os.remove(output_path)\n",
        "            print(f\"Deleted old version of {filename}\")\n",
        "\n",
        "        save_file(state_dict, output_path)\n",
        "        print(f\"Model saved to: {output_path}\")\n",
        "\n",
        "        # Get shareable link\n",
        "        auth.authenticate_user()\n",
        "        service = build('drive', 'v3')\n",
        "\n",
        "        results = service.files().list(\n",
        "            q=f\"name='{filename}'\",\n",
        "            fields='files(id)'\n",
        "        ).execute()\n",
        "\n",
        "        if results.get('files'):\n",
        "            file_id = results['files'][0]['id']\n",
        "            service.permissions().create(\n",
        "                fileId=file_id,\n",
        "                body={'type': 'anyone', 'role': 'reader'}\n",
        "            ).execute()\n",
        "            link = f\"https://drive.google.com/file/d/{file_id}/view?usp=sharing\"\n",
        "            print(f\"Shareable link: {link}\")\n",
        "            return link\n",
        "    else:\n",
        "        # Local: Save to models/safetensors\n",
        "        output_dir = 'models/safetensors'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        output_path = os.path.join(output_dir, filename)\n",
        "\n",
        "        save_file(state_dict, output_path)\n",
        "        print(f\"Model saved to: {output_path}\")\n",
        "        return output_path\n",
        "\n",
        "    return None"
      ],
      "id": "c5d48e718fda1d1b"
    },
    {
      "cell_type": "markdown",
      "id": "ucfjvvw8pdc",
      "source": [
        "## Step 3: Save VAE with EMA Weights"
      ],
      "metadata": {
        "id": "ucfjvvw8pdc"
      }
    },
    {
      "cell_type": "code",
      "id": "zs7mlg1zqze",
      "source": [
        "# Extract and save EMA weights\n",
        "with ema.average_parameters():\n",
        "    vae_state_dict = {k: v.cpu().clone() for k, v in vae.state_dict().items()}\n",
        "\n",
        "vae_link = save_and_share(vae_state_dict, filename='vae_flat_16_ema.safetensors')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zs7mlg1zqze",
        "outputId": "76ce04d6-5355-4d44-8dd2-76072543e3ae"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Model saved to: /content/drive/MyDrive/vae_models/vae_flat_16_ema.safetensors\n",
            "Shareable link: https://drive.google.com/file/d/10Fnxhl1JznXRtTVVqVJWgKWMP4RgbXAW/view?usp=sharing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c5fb97796c19acb"
      },
      "source": [
        "## Step 4: Encode MNIST Dataset"
      ],
      "id": "3c5fb97796c19acb"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c727c5c53c90a4a6",
        "outputId": "ce2236a6-cdf4-4ead-ba47-467cba982cc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoding dataset: 100%|██████████| 469/469 [00:07<00:00, 62.52it/s]\n",
            "Encoding dataset: 100%|██████████| 79/79 [00:01<00:00, 58.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded train shape: torch.Size([60000, 16])\n",
            "Encoded test shape: torch.Size([10000, 16])\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "import gc\n",
        "\n",
        "# Clear GPU memory before encoding\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "def encode_dataset(model, dataloader, ema=None):\n",
        "    model.eval()\n",
        "    latents = []\n",
        "    labels = []\n",
        "\n",
        "    context = ema.average_parameters() if ema is not None else torch.no_grad()\n",
        "\n",
        "    with context:\n",
        "        with torch.no_grad():\n",
        "            for data, label in tqdm(dataloader, desc=\"Encoding dataset\"):\n",
        "                data = data.to(device)\n",
        "                mu, _ = model.encoder(data)\n",
        "                latents.append(mu.cpu())\n",
        "                labels.append(label)\n",
        "\n",
        "                # Clear cache periodically\n",
        "                if len(latents) % 50 == 0:\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "    latents = torch.cat(latents, dim=0)\n",
        "    labels = torch.cat(labels, dim=0)\n",
        "    return TensorDataset(latents, labels)\n",
        "\n",
        "# Encode both train and test sets\n",
        "encoded_train = encode_dataset(vae, train_loader, ema=ema)\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "encoded_test = encode_dataset(vae, val_loader, ema=ema)\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(f\"Encoded train shape: {encoded_train.tensors[0].shape}\")\n",
        "print(f\"Encoded test shape: {encoded_test.tensors[0].shape}\")\n",
        "\n",
        "# Create dataloaders for flow training\n",
        "flow_train_loader = DataLoader(encoded_train, batch_size=flow_train_config['batch_size'], shuffle=True)\n",
        "flow_val_loader = DataLoader(encoded_test, batch_size=flow_train_config['batch_size'], shuffle=False)"
      ],
      "id": "c727c5c53c90a4a6"
    },
    {
      "metadata": {
        "id": "d913d46509a9510d"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 5: Train Flow Model"
      ],
      "id": "d913d46509a9510d"
    },
    {
      "cell_type": "code",
      "id": "9mk2ehgyrz",
      "source": [
        "# Initialize wandb for flow training\n",
        "wandb.init(project=\"reproduce_final_submission\", config=flow_train_config)\n",
        "\n",
        "# Create flow model\n",
        "flow_model = FlatVelocityNet(input_dim=16, h_dim=64).to(device)\n",
        "total_params = sum(p.numel() for p in flow_model.parameters())\n",
        "print(f\"Flow model parameters: {total_params:,}\")\n",
        "\n",
        "wandb.config.update({\n",
        "    \"total_params\": total_params,\n",
        "    \"model_class\": flow_model.__class__.__name__,\n",
        "    \"input_dim\": 16,\n",
        "    \"h_dim\": 64\n",
        "})\n",
        "\n",
        "# Training setup\n",
        "criterion = nn.MSELoss()\n",
        "optimizer_flow = optim.Adam(flow_model.parameters(), lr=flow_train_config['learning_rate'])\n",
        "ema_flow = ExponentialMovingAverage(flow_model.parameters(), decay=0.9999)\n",
        "\n",
        "global_step = 0\n",
        "spatial = False  # Flat latents, not spatial\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(flow_train_config['epochs']):\n",
        "    flow_model.train()\n",
        "\n",
        "    pbar = tqdm(flow_train_loader, desc=f\"Flow Epoch {epoch+1}/{flow_train_config['epochs']}\", leave=False)\n",
        "    for batch_idx, (data, _) in enumerate(pbar):\n",
        "        optimizer_flow.zero_grad()\n",
        "\n",
        "        # Get data and create noise sample\n",
        "        target_x = data.to(device)\n",
        "        sampled_x = torch.randn_like(target_x)\n",
        "        B = sampled_x.size(0)\n",
        "\n",
        "        # Sample random timestep\n",
        "        t = torch.rand(B, 1, device=device, dtype=target_x.dtype)\n",
        "\n",
        "        # Interpolate between noise and data\n",
        "        if spatial:\n",
        "            t4 = t.view(B, 1, 1, 1)\n",
        "            interpolated_x = sampled_x * (1 - t4) + target_x * t4\n",
        "        else:\n",
        "            interpolated_x = sampled_x * (1 - t) + target_x * t\n",
        "\n",
        "        # Compute target direction\n",
        "        line_directions = target_x - sampled_x\n",
        "\n",
        "        # Predict drift\n",
        "        drift = flow_model(interpolated_x, t)\n",
        "        loss = criterion(drift, line_directions)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer_flow.step()\n",
        "        ema_flow.update()\n",
        "\n",
        "        # Metrics\n",
        "        with torch.no_grad():\n",
        "            cos_sim = F.cosine_similarity(drift, line_directions, dim=1).mean()\n",
        "            drift_norm = drift.norm(dim=1).mean()\n",
        "\n",
        "        wandb.log({\n",
        "            \"step\": global_step,\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": loss.item(),\n",
        "            \"cos_sim\": cos_sim.item(),\n",
        "            \"drift_norm\": drift_norm.item(),\n",
        "        })\n",
        "        global_step += 1\n",
        "        pbar.set_postfix(Loss=f\"{loss.item():.4f}\", CosSim=f\"{cos_sim.item():.3f}\", Drift=f\"{drift_norm.item():.3f}\")\n",
        "\n",
        "    # Validation every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        flow_model.eval()\n",
        "        with ema_flow.average_parameters():\n",
        "            val_loss = 0.0\n",
        "            val_cos_sim = 0.0\n",
        "            n_batches = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for data, _ in flow_val_loader:\n",
        "                    target_x = data.to(device)\n",
        "                    sampled_x = torch.randn_like(target_x)\n",
        "                    B = sampled_x.size(0)\n",
        "\n",
        "                    t = torch.rand(B, 1, device=device, dtype=target_x.dtype)\n",
        "\n",
        "                    if spatial:\n",
        "                        t4 = t.view(B, 1, 1, 1)\n",
        "                        interpolated_x = sampled_x * (1 - t4) + target_x * t4\n",
        "                    else:\n",
        "                        interpolated_x = sampled_x * (1 - t) + target_x * t\n",
        "\n",
        "                    line_directions = target_x - sampled_x\n",
        "                    drift = flow_model(interpolated_x, t)\n",
        "                    loss = criterion(drift, line_directions)\n",
        "                    cos_sim = F.cosine_similarity(drift, line_directions, dim=1).mean()\n",
        "\n",
        "                    val_loss += loss.item()\n",
        "                    val_cos_sim += cos_sim.item()\n",
        "                    n_batches += 1\n",
        "\n",
        "            wandb.log({\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"val_loss\": val_loss / n_batches,\n",
        "                \"val_cos_sim\": val_cos_sim / n_batches,\n",
        "            })\n",
        "            print(f\"Epoch {epoch+1} - Val Loss: {val_loss/n_batches:.4f}, CosSim: {val_cos_sim/n_batches:.3f}\")\n",
        "\n",
        "print(\"Flow training complete!\")\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "9mk2ehgyrz",
        "outputId": "122f0478-0932-4a92-dd6f-96e98743b206"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251012_234259-klgy1mj4</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/marcocassar-belmont-university/reproduce_final_submission/runs/klgy1mj4' target=\"_blank\">cosmic-glade-5</a></strong> to <a href='https://wandb.ai/marcocassar-belmont-university/reproduce_final_submission' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/marcocassar-belmont-university/reproduce_final_submission' target=\"_blank\">https://wandb.ai/marcocassar-belmont-university/reproduce_final_submission</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/marcocassar-belmont-university/reproduce_final_submission/runs/klgy1mj4' target=\"_blank\">https://wandb.ai/marcocassar-belmont-university/reproduce_final_submission/runs/klgy1mj4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flow model parameters: 10,512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flow training complete!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>cos_sim</td><td>▁▃▃▃▄▄▅▆▆▇▇▇▇▇█▇▇▇▇▇▇█▇███▇████▇█▇▇████▇</td></tr><tr><td>drift_norm</td><td>▁▁▁▁▂▃▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇███▇▇█████</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▂▂▂▂▂▂▃▃▃▃▄▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇█████</td></tr><tr><td>train_loss</td><td>▆█▆▇▆▇▅▅▄▃▃▃▃▂▂▃▃▃▂▂▂▂▂▂▁▂▂▂▂▃▃▁▁▁▁▂▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>cos_sim</td><td>0.58605</td></tr><tr><td>drift_norm</td><td>3.06517</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>step</td><td>468</td></tr><tr><td>train_loss</td><td>1.18631</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">cosmic-glade-5</strong> at: <a href='https://wandb.ai/marcocassar-belmont-university/reproduce_final_submission/runs/klgy1mj4' target=\"_blank\">https://wandb.ai/marcocassar-belmont-university/reproduce_final_submission/runs/klgy1mj4</a><br> View project at: <a href='https://wandb.ai/marcocassar-belmont-university/reproduce_final_submission' target=\"_blank\">https://wandb.ai/marcocassar-belmont-university/reproduce_final_submission</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251012_234259-klgy1mj4/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 17
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e35f87c9109bc467",
        "outputId": "5aac68a1-8d64-42b6-c225-ec8a543112da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to: /content/drive/MyDrive/vae_models/flow_flat_16_ema.safetensors\n",
            "Flow model saved!\n"
          ]
        }
      ],
      "source": [
        "# Extract and save EMA weights\n",
        "with ema_flow.average_parameters():\n",
        "    flow_state_dict = {k: v.cpu().clone() for k, v in flow_model.state_dict().items()}\n",
        "\n",
        "flow_link = save_and_share(flow_state_dict, filename='flow_flat_16_ema.safetensors')\n",
        "print(\"Flow model saved!\")"
      ],
      "id": "e35f87c9109bc467"
    },
    {
      "cell_type": "markdown",
      "id": "fjaku2l3mvv",
      "source": [
        "## Step 7: Generate Samples"
      ],
      "metadata": {
        "id": "fjaku2l3mvv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "1cd10745681e32fa",
        "outputId": "eb3ce331-7978-4aa6-aed4-e035670da8d1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 8 samples\n",
            "Total parameters (VAE + Flow): 169905\n"
          ]
        }
      ],
      "source": [
        "# Generate samples\n",
        "n_samples = 8\n",
        "n_steps = 15\n",
        "\n",
        "# Move models back to device if needed\n",
        "vae = vae.to(device)\n",
        "flow_model = flow_model.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    with ema_flow.average_parameters():\n",
        "        # Start from noise\n",
        "        z_noise = torch.randn(n_samples, 16, device=device)\n",
        "\n",
        "        # Integrate through flow to get latents\n",
        "        z_samples = integrate_path(flow_model, z_noise, step_fn=rk4_step, n_steps=n_steps, latent_2d=False)\n",
        "\n",
        "    with ema.average_parameters():\n",
        "        # Decode latents to images\n",
        "        x_samples = vae.decoder(z_samples)\n",
        "        x_samples = torch.sigmoid(x_samples)\n",
        "        x_samples = x_samples.cpu()\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(1, 8, figsize=(10, 10))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(x_samples[i, 0], cmap='gray')\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.savefig('generated_samples.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Generated {n_samples} samples\")\n",
        "print(\"Total parameters (VAE + Flow):\", sum(p.numel() for p in vae.parameters()) + sum(p.numel() for p in flow_model.parameters()))"
      ],
      "id": "1cd10745681e32fa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9bbcda8e79484f5"
      },
      "source": [
        "## Submission Interface Class\n",
        "\n",
        "The following class is used for automated evaluation and can load models from Google Drive links."
      ],
      "id": "a9bbcda8e79484f5"
    },
    {
      "cell_type": "code",
      "id": "1s9cno8wqcc",
      "source": [
        "!pip install gdown\n",
        "\n",
        "import gdown\n",
        "\n",
        "class SubmissionInterface(nn.Module):\n",
        "    \"\"\"All teams must implement this for automated evaluation.\n",
        "    When you subclass/implement these methods, replace the NotImplementedError.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # --- REQUIRED INFO:\n",
        "        self.info = {\n",
        "            'team': 'marco',\n",
        "            'names': 'Marco',\n",
        "        }\n",
        "        self.latent_dim = 16\n",
        "        self.specifications = {\n",
        "            \"latent_shape\": (16,),\n",
        "            \"base_channels\": 16,\n",
        "            \"blocks_per_level\": 2,\n",
        "            \"groups\": 4,\n",
        "            \"dropout\": 0.3,\n",
        "            \"act\": nn.GELU,\n",
        "            \"use_skips\": True,\n",
        "            \"use_bn\": True\n",
        "        }\n",
        "        # ----\n",
        "\n",
        "        # keep support for full auto-initialization:\n",
        "        self.device = 'cpu'\n",
        "        self.load_vae()\n",
        "        self.load_flow_model()\n",
        "\n",
        "    def load_vae(self):\n",
        "        \"\"\"this completely specifies the vae model including configuration parameters,\n",
        "            downloads/mounts the weights from Google Drive, automatically loads weights\"\"\"\n",
        "        self.vae = InspoResNetVAE(**self.specifications)\n",
        "        vae_weights_file = 'downloaded_vae.safetensors'\n",
        "        safetensors_link = \"https://drive.google.com/file/d/1rP6yP5yixCI1M7LOrv9v9vJkeYkXnptG/view?usp=drive_link\"\n",
        "        gdown.download(safetensors_link, vae_weights_file, quiet=False, fuzzy=True)\n",
        "        self.vae.load_state_dict(load_file(vae_weights_file))\n",
        "\n",
        "    def load_flow_model(self):\n",
        "        \"\"\"this completely specifies the flow model including configuration parameters,\n",
        "           downloads/mounts the weights from Google Drive, automatically loads weights\"\"\"\n",
        "        self.flow_model = FlatVelocityNet(input_dim=self.latent_dim)\n",
        "        flow_weights_file = 'downloaded_flow.safetensors'\n",
        "        safetensors_link = \"https://drive.google.com/file/d/1pAU4p6xQMDgAuywu-go-5iM4qPDXgJFc/view?usp=drive_link\"\n",
        "        gdown.download(safetensors_link, flow_weights_file, quiet=False, fuzzy=True)\n",
        "        self.flow_model.load_state_dict(load_file(flow_weights_file))\n",
        "\n",
        "    def generate_samples(self, n_samples: int, n_steps=15) -> torch.Tensor:\n",
        "        z0 = torch.randn([n_samples, self.latent_dim]).to(self.device)\n",
        "        z1 = integrate_path(self.flow_model, z0, n_steps=n_steps, step_fn=rk4_step)\n",
        "        gen_xhat = F.sigmoid(self.decode(z1).view(-1, 28, 28))\n",
        "        return gen_xhat\n",
        "\n",
        "    def encode(self, images: torch.Tensor) -> torch.Tensor:\n",
        "        # if your vae has linear layers, flatten first\n",
        "        # if your vae has conv layers, comment out next line\n",
        "        # images = images.view(images.size(0), -1)\n",
        "        with torch.no_grad():\n",
        "            z = self.vae.encoder(images.to(self.device))\n",
        "            # mu = z[:, :self.latent_dim]  # return only first half (mu)\n",
        "            if isinstance(z, (tuple, list)):\n",
        "                mu, _ = z\n",
        "            else:\n",
        "                mu = z\n",
        "            return mu\n",
        "\n",
        "    def decode(self, latents: torch.Tensor) -> torch.Tensor:\n",
        "        return self.vae.decoder(latents)\n",
        "\n",
        "    def to(self, device):\n",
        "        self.device = device\n",
        "        self.vae.to(self.device)\n",
        "        self.flow_model.to(self.device)\n",
        "        return self\n",
        "\n",
        "# Sample usage:\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
        "# mysub = SubmissionInterface().to(device) # loads vae and flow models\n",
        "# xhat_gen = mysub.generate_samples(n_samples=10, n_steps=100)"
      ],
      "metadata": {
        "id": "1s9cno8wqcc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}